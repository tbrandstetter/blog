{
    "version": "https://jsonfeed.org/version/1",
    "title": "No Rocket Science",
    "description": "",
    "home_page_url": "https://norocketscience.at",
    "feed_url": "https://norocketscience.at/feed.json",
    "user_comment": "",
    "icon": "https://norocketscience.at/media/website/logo_dark.png",
    "author": {
        "name": "Thomas Brandstetter"
    },
    "items": [
        {
            "id": "https://norocketscience.at/highly-separated-talos-kubernetes-cluster-part3-pfsense-installation/",
            "url": "https://norocketscience.at/highly-separated-talos-kubernetes-cluster-part3-pfsense-installation/",
            "title": "Highly separated Talos Kubernetes Cluster: Part 3 - Registry installation",
            "summary": "Welcome back to another part of building an highly separated Talos Kubernetes cluster! In this part we will install the internal registry which avoids the direct connection to the internet for the Talos cluster. Furthermore, the Talos image factory will be hosted on premise too,&hellip;",
            "content_html": "\n  <p>\n    Welcome back to another part of building an highly separated Talos Kubernetes cluster! In this part we will install the internal registry which avoids the direct connection to the internet for the Talos cluster. Furthermore, the Talos image factory will be hosted on premise too, which allows us to create Talos boot images with own secure-boot key locally.\n  </p>\n\n  <p>\n    All installation steps in this post will be taken on the registry host!\n  </p>\n\n    <h2 id=\"install-docker\">\n      Install Docker\n    </h2>\n\n  <p>\n    Harbor is going to be installed as a Docker container. Thus, we need to install Docker first on the Registry host. I'm using the <a href=\"https://docs.docker.com/engine/install/ubuntu/\">official Docker guide</a> for the installation.\n  </p>\n\n  <p>\n    Add the repository:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>sudo apt update\nsudo apt install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\nsudo tee /etc/apt/sources.list.d/docker.sources &lt;&lt;EOF\nTypes: deb\nURIs: https://download.docker.com/linux/ubuntu\nSuites: $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\")\nComponents: stable\nSigned-By: /etc/apt/keyrings/docker.asc\nEOF\n\nsudo apt update\n</code></pre>\n\n  <p>\n    Install the necessary packages:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</code></pre>\n\n  <p>\n    Verify that Docker is up and running:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>sudo systemctl status docker\n\n# if it isn't running start it\nsudo systemctl start docker\n</code></pre>\n\n  <p>\n    Add the ubuntu user to the docker group (you have to re-login afterwards):\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>sudo usermod -a -G docker ubuntu</code></pre>\n\n    <h2 id=\"create-self-signed-certificates\">\n      Create self-signed certificates\n    </h2>\n\n  <p>\n    To communicate through https between Talos and the registry we're going to use self-signed certificates:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>mkdir $HOME/certs && cd $HOME/certs\n\n# generate the CA private key\nopenssl genrsa -out ca.key 4096\n\n# generate the CA certificate\nopenssl req -x509 -new -nodes -sha512 -days 3650 -subj \"/C=CN/ST=NA/L=NA/O=homelab/OU=PKI/CN=HOMELAB CA\" -key ca.key -out ca.crt\n\n# generate the server certificate private key\nopenssl genrsa -out registry.mgmt.lab.internal.key 4096\n\n# generate server certificate signing request\nopenssl req -sha512 -new -subj \"/C=CN/ST=NA/L=NA/O=homelab/OU=PKI/CN=registry.mgmt.lab.internal\" -key registry.mgmt.lab.internal.key -out registry.mgmt.lab.internal.csr\n\n# generate an x509 v3 extension file\ncat &gt; v3.ext &lt;&lt;-EOF\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nextendedKeyUsage = serverAuth\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1=registry.mgmt.lab.internal\nDNS.2=registry.mgmt.lab\nDNS.3=registry\nEOF\n\n# generate the server certificate\nopenssl x509 -req -sha512 -days 3650 -extfile v3.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in registry.mgmt.lab.internal.csr -out registry.mgmt.lab.internal.crt\n\n# copy certificates and keys to own directory for Harbor installation\nsudo mkdir -p /data/cert && sudo cp registry.mgmt.lab.internal.crt registry.mgmt.lab.internal.key /data/cert/\n</code></pre>\n\n    <h2 id=\"installing-harbor-registry\">\n      Installing Harbor registry\n    </h2>\n\n  <p>\n    Download and extract the latest Harbor offline installer from&nbsp;<a href=\"https://github.com/goharbor/harbor/releases\">https://github.com/goharbor/harbor/releases</a>\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>wget https://github.com/goharbor/harbor/releases/download/v2.14.2/harbor-offline-installer-v2.14.2.tgz\ntar xzvf harbor-offline-installer-v2.14.2.tgz\n</code></pre>\n\n  <p>\n    Next we have to configure Harbor through the harbor.yml file:\n  </p>\n<pre class=\"line-numbers  language-yaml\"><code>hostname: registry.mgmt.lab.internal\nhttps:\n  port: 443\n  certificate: /data/cert/registry.mgmt.lab.internal.crt\n  private_key: /data/cert/registry.mgmt.lab.internal.key\n\nharbor_admin_password: Harbor12345\n\ndatabase:\n  password: root123\n  max_idle_conns: 100\n  max_open_conns: 900\n  conn_max_lifetime: 5m\n  conn_max_idle_time: 0\n\ndata_volume: /data\n\njobservice:\n  max_job_workers: 10\n  max_job_duration_hours: 24\n  job_loggers:\n    - STD_OUTPUT\n    - FILE\n  logger_sweeper_duration: 1 #days\n\nnotification:\n  webhook_job_max_retry: 3\n  webhook_job_http_client_timeout: 3 #seconds\n\nlog:\n  level: info\n  local:\n    rotate_count: 50\n    rotate_size: 200M\n    location: /var/log/harbor\n\n#This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY!\n_version: 2.14.0\n\nupload_purging:\n  age: 168h\n  interval: 24h\n  dryrun: false\n\ncache:\n  enabled: false\n  expire_hours: 24</code></pre>\n\n  <p>\n    Finally, we can install the Harbor registry using the offline installer:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>cd harbor && sudo ./install.sh\n</code></pre>\n\n  <p>\n    If everything went fine you should see the docker container running with docker ps:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/harbor1.jpg\" height=\"264\" width=\"2734\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/harbor1-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/harbor1-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/harbor1-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/harbor1-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/harbor1-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/harbor1-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Go to https://registry.mgt.lab.internal and you should see the administration interface:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/harbor2.jpg\" height=\"1316\" width=\"2574\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/harbor2-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/harbor2-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/harbor2-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/harbor2-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/harbor2-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/harbor2-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    The password is the same as from the harbor.yaml\n  </p>\n\n  <p>\n    Be aware that on a reboot you have to manually start the service with docker compose in the harbor directory. If you like to autostart the service, install the systemd service file from&nbsp;<a href=\"https://github.com/goharbor/harbor/blob/main/tools/systemd/harbor.service\">https://github.com/goharbor/harbor/blob/main/tools/systemd/harbor.service</a>&nbsp;and copy the docker compose file to /etc/goharbor/harbor/docker-compose.yml\n  </p>\n\n    <h2 id=\"configure-proxy-caches\">\n      Configure proxy caches\n    </h2>\n\n  <p>\n    With the running Harbor registry we can create the needed proxy caches for Talos. The following registries have to be configured:\n  </p>\n\n  <p>\n    Provider: Docker Hub<br>Name: docker.io<br>Endpoint Url: https://hub.docker.com<br>Access ID: leave empty<br>Access Secret: leave empty<br>Verify Remote Cert: Checked<br><br>Provider: Docker Registry<br>Name: gcr.io<br>Endpoint Url: https://gcr.io<br>Access ID: leave empty<br>Access Secret: leave empty<br>Verify Remote Cert: Checked<br><br>Provider: Docker Registry<br>Name: quay.io<br>Endpoint Url: https://quay.io<br>Access ID: leave empty<br>Access Secret: leave empty<br>Verify Remote Cert: Checked<br><br>Provider: Docker Registry<br>Name: registry.k8s.io<br>Endpoint Url: https://registry.k8s.io<br>Access ID: leave empty<br>Access Secret: leave empty<br>Verify Remote Cert: Checked<br><br>Provider: Github GHCR<br>Name: ghcr.io<br>Endpoint Url: https://ghcr.io<br>Access ID: leave empty<br>Access Secret: leave empty<br>Verify Remote Cert: Checked<br><br>In the gui it looks like this:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/harbor3.jpg\" height=\"1020\" width=\"1134\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/harbor3-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/harbor3-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/harbor3-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/harbor3-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/harbor3-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/harbor3-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Afterwards we're mapping the repositories as proxy cache. Go to the projects tab and click on \"New project\". Fill out the name with prefix \"proxy-\". Set the Access Level to public to avoid authentication from the Talos nodes and select the right repository in Proxy Cache. Repeat this step for all available repositories you created.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/habor.jpg\" height=\"968\" width=\"1080\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/habor-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/habor-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/habor-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/habor-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/habor-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/habor-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Additionally, we need a \"siderolabs\" project for the Talos Image factory. This is setup as a plain project without a Proxy Cache. Still Public as the other ones. Finally, the projects should look like this:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/harbor2-3.jpg\" height=\"426\" width=\"1330\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/harbor2-3-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/harbor2-3-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/harbor2-3-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/harbor2-3-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/harbor2-3-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/harbor2-3-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <h2 id=\"installing-talos-image-factory\">\n      Installing Talos Image Factory\n    </h2>\n\n  <p>\n    The image factory is needed to create an boot image locally with your own secure boot keys. Talos has a own dedicated service for this too, but for this blog series we're going fully on premise for the bootstrap images.\n  </p>\n\n  <p>\n    \n  </p>\n\n    <h3 id=\"prerequisites\">\n      Prerequisites\n    </h3>\n\n  <ul>\n    <li><a href=\"https://github.com/google/go-containerregistry/blob/main/cmd/crane/README.md\">Crane</a></li><li>Signing Key for co signing:&nbsp;<code>openssl ecparam -name prime256v1 -genkey -noout -out signing-key.key</code></li><li><a href=\"https://www.talos.dev\">Talosctl</a> on the Management Client</li>\n  </ul>\n\n  <p>\n    Get the Talos image list and copy the txt file from the management to the registry host:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>talosctl image talos-bundle v1.12.1 &gt; images.txt</code></pre>\n\n  <p>\n    Copy the necessary images from siderolabs to internal registry with the following script:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>REGISTRY_ENDPOINT=registry.mgmt.lab.internal\ncrane auth login ${REGISTRY_ENDPOINT} -u admin -p Harbor12345\nfor SOURCE_IMAGE in $(cat images.txt)\n  do\n    IMAGE_WITHOUT_DIGEST=${SOURCE_IMAGE%%@*}\n    IMAGE_WITH_NEW_REG=\"${REGISTRY_ENDPOINT}/${IMAGE_WITHOUT_DIGEST#*/}\"\n    crane --insecure copy \\\n      $SOURCE_IMAGE \\\n      $IMAGE_WITH_NEW_REG\ndone</code></pre>\n\n  <p>\n    Create cosign key with:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>sudo docker run --rm -it \\\n-v $PWD:/keys -w /keys \\\n-e COSIGN_PASSWORD=\"\" \\\n--user $(id -u):$(id -g) \\\nghcr.io/sigstore/cosign/cosign:v2.6.1 \\\ngenerate-key-pair</code></pre>\n\n  <p>\n    Sign Images:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>KEY_FILE=\"cosign.key\"\nREGISTRY_ENDPOINT=registry.mgmt.lab.internal\ncrane auth login ${REGISTRY_ENDPOINT} -u admin -p Harbor12345\n\nfor IMAGE in $(cat images.txt)\n  do\n    NEW_IMAGE=\"${REGISTRY_ENDPOINT}/${IMAGE#*/}\"\n    if [[ \"$NEW_IMAGE\" != *\"@sha256:\"* ]]; then\n      NEW_IMAGE=\"${NEW_IMAGE}@$(crane --insecure digest $NEW_IMAGE)\"\n    fi\n    docker run --rm -it --net=host \\\n      -v $PWD:/keys -w /keys \\\n      -v $HOME/.docker/config.json:/.docker/config.json:ro \\\n      -e DOCKER_CONFIG=/.docker \\\n      -e COSIGN_PASSWORD=\"\" \\\n      -e COSIGN_YES=true \\\n      --user $(id -u):$(id -g) \\\n      docker.io/bitnami/cosign:latest \\\n        sign \\\n\t--allow-insecure-registry \\\n\t--key /keys/$KEY_FILE \\\n        --tlog-upload=false \\\n\t--use-signing-config=false \\\n\t--new-bundle-format=false \\\n        $NEW_IMAGE\ndone\n</code></pre>\n\n  <p>\n    Generate SecureBoot keys/certs with:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>talosctl gen secureboot pcr (PCR Key)\ntalosctl gen secureboot uki --common-name \"SecureBoot Key\" (Signing Key and Cert)\n</code></pre>\n\n  <p>\n    <br>copy the&nbsp;<code>pcr-signing-key.pem</code>,&nbsp;<code>uki-signing-key.pem</code> and&nbsp;<code>uki-signing-cert.pem</code> to the registry host\n  </p>\n\n  <p>\n    Create the configuration for the Talos image factory with a env.yml file:\n  </p>\n<pre class=\"line-numbers  language-yaml\"><code>#env.yml\nartifacts:\n    core:\n        components:\n            extensionManifest: siderolabs/extensions\n            imager: siderolabs/imager\n            installer: siderolabs/installer\n            installerBase: siderolabs/installer-base\n            overlayManifest: siderolabs/overlays\n            talosctl: siderolabs/talosctl-all\n        insecure: false\n        registry: registry.mgmt.lab.internal\n    installer:\n        external:\n            insecure: false\n            namespace: \"\"\n            registry: \"\"\n            repository: \"\"\n        internal:\n            insecure: false\n            namespace: siderolabs\n            registry: registry.mgmt.lab.internal\n            repository: \"\"\n    refreshInterval: 5m0s\n    schematic:\n        insecure: false\n        namespace: siderolabs/image-factory\n        registry: registry.mgmt.lab.internal\n        repository: schematics\n    talosVersionRecheckInterval: 15m0s\nbuild:\n    maxConcurrency: 6\n    minTalosVersion: 1.2.0\ncache:\n    cdn:\n        enabled: false\n        host: \"\"\n        trimPrefix: \"\"\n    oci:\n        insecure: false\n        namespace: siderolabs/image-factory\n        registry: registry.mgmt.lab.internal\n        repository: cache\n    s3:\n        bucket: image-factory\n        enabled: false\n        endpoint: \"\"\n        insecure: false\n        region: \"\"\n    signingKeyPath: \"/signing-key.key\"\ncontainerSignature:\n    disabled: false\n    issuer: https://accounts.google.com\n    issuerRegExp: \"\"\n    publicKeyFile: \"/cosign.pub\"\n    publicKeyHashAlgo: sha256\n    subjectRegExp: '@siderolabs\\.com$'\nhttp:\n    allowedOrigins:\n        - '*'\n    certFile: \"/certs/server-chain.pem\"\n    externalPXEURL: \"\"\n    externalURL: https://registry.mgmt.lab.internal:9443/\n    httpListenAddr: :8080\n    keyFile: \"/certs/server-key.pem\"\nmetrics:\n    addr: :2122\nsecureBoot:\n    awsKMS:\n        certARN: \"\"\n        certPath: \"\"\n        keyID: \"\"\n        pcrKeyID: \"\"\n        region: \"\"\n    azureKeyVault:\n        certificateName: \"\"\n        keyName: \"\"\n        url: \"\"\n    enabled: true\n    file:\n        pcrKeyPath: \"./pcr-signing-key.pem\"\n        signingCertPath: \"./uki-signing-cert.pem\"\n        signingKeyPath: \"./uki-signing-key.pem\"</code></pre>\n\n  <p>\n    Start Talos Image Factory with docker-compose:\n  </p>\n<pre class=\"line-numbers  language-yaml\"><code>services:\n  image-factory:\n    image: \"ghcr.io/siderolabs/image-factory:v1.0.0-beta.0\"\n    restart: unless-stopped\n    container_name: image-factory\n    ports:\n      - \"9443:8080\"\n    volumes:\n      - $HOME/.docker/config.json:/.docker/config.json\n      - ./env.yaml:/env.yaml\n      - ./pcr-signing-key.pem:/pcr-signing-key.pem\n      - ./uki-signing-key.pem:/uki-signing-key.pem\n      - ./uki-signing-cert.pem:/uki-signing-cert.pem\n      - ./signing-key.key:/signing-key.key\n      - ./cosign.pub:/cosign.pub\n      # CA cert for Habor verification\n      - ./ca.crt:/etc/pki/tls/cacert.pem\n      - /data/cert/registry.mgmt.lab.internal.key:/certs/server-key.pem\n      - /data/cert/registry.mgmt.lab.internal.crt:/certs/server-chain.pem\n    command: \n      --config env.yaml</code></pre>\n\n    <h2 id=\"generate-the-bootstrap-image\">\n      Generate the bootstrap image\n    </h2>\n\n  <p>\n    Go to <a href=\"https://registry.mgmt.lab.internal:9443\">https://registry.mgmt.lab.internal:9443</a> and set the following options:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/talosif.jpg\" height=\"764\" width=\"1112\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/talosif-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/talosif-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/talosif-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/talosif-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/talosif-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/talosif-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/talosif2.jpg\" height=\"811\" width=\"1417\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/talosif2-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/talosif2-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/talosif2-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/talosif2-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/talosif2-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/talosif2-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/talosif3.jpg\" height=\"811\" width=\"1417\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/talosif3-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/talosif3-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/talosif3-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/talosif3-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/talosif3-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/talosif3-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p id=\"summary\">\n    Upload the image as proxmox iso image. We're going to use that to bootstrap the Talos cluster in the next session\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/9/talosif4.jpg\" height=\"229\" width=\"1813\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/9/responsive/talosif4-xs.jpg 640w ,https://norocketscience.at/media/posts/9/responsive/talosif4-sm.jpg 768w ,https://norocketscience.at/media/posts/9/responsive/talosif4-md.jpg 1024w ,https://norocketscience.at/media/posts/9/responsive/talosif4-lg.jpg 1366w ,https://norocketscience.at/media/posts/9/responsive/talosif4-xl.jpg 1600w ,https://norocketscience.at/media/posts/9/responsive/talosif4-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <h2 id=\"summary\">\n      Summary\n    </h2>\n\n  <p>\n    We now have a running registry using Harbor and a Talos Image Factory on premise to create local Talos bootstrap images with a own secure boot key. In the next post we will configure and install the Talos cluster.\n  </p>\n\n  <p>\n    That's all for now. Stay tuned and leave me a comment on <a href=\"https://tinyurl.com/4atbveyr\">LinkedIn</a> for any problems or suggestions you have.\n  </p>\n\n  <p>\n    Related posts:\n  </p>\n\n  <ul>\n    <li><a href=\"https://norocketscience.at/supercharged-air-gapped-talos-kubernetes-cluster-an-overview/\" style=\"font-size: 1em;\">Highly separated Talos Kubernetes Cluster: Part1 - Overview</a></li><li><a href=\"https://norocketscience.at/highly-separated-talos-kubernetes-cluster-part2-vm-installation/\" style=\"font-size: 1em;\">Highly separated Talos Kubernetes Cluster: Part 2 - VM installation</a></li>\n  </ul>\n\n  <p>\n    \n  </p>",
            "author": {
                "name": "Thomas Brandstetter"
            },
            "tags": [
            ],
            "date_published": "2026-02-25T21:07:00+01:00",
            "date_modified": "2026-02-26T08:10:04+01:00"
        },
        {
            "id": "https://norocketscience.at/highly-separated-talos-kubernetes-cluster-part2-vm-installation/",
            "url": "https://norocketscience.at/highly-separated-talos-kubernetes-cluster-part2-vm-installation/",
            "title": "Highly separated Talos Kubernetes Cluster: Part 2 - VM installation",
            "summary": "Welcome back to our series on building an highly separated Talos Kubernetes cluster! In this second instalment, i'll focus on preparing your Proxmox environment. This involves creating a Proxmox image template for the Ubuntu VMs, downloading necessary images for the management client, and setting up&hellip;",
            "content_html": "\n  <p>\n    Welcome back to our series on building an highly separated Talos Kubernetes cluster! In this second instalment, i'll focus on preparing your Proxmox environment. This involves creating a Proxmox image template for the Ubuntu VMs, downloading necessary images for the management client, and setting up the network bridges for the different networks.\n  </p>\n\n    <h2 id=\"system-requirements-for-proxmox\">\n      System requirements for Proxmox\n    </h2>\n\n  <p>\n    I used an old Thinkcentre M910 from 2018 for this lab. So the requirements are really low level. A decent amount of ram (32GB) and a not too old cpu should be doing it. The Thinkcentre has only one network interface, so I'm working with vlan's in my setup. If you have two or more interface, you can split the incoming networks (client and server). It depends on your own configuration.\n  </p>\n\n  <p>\n    All configuration needed for this part (excluding the Ubuntu Desktop installation) can be found in my <a href=\"https://github.com/tbrandstetter/norocketscience-examples/tree/main/talos-kubernetes-cluster-part2\">GIT repository</a>.\n  </p>\n\n    <h2 id=\"creating-a-proxmox-image-templatelessbrgreater\">\n      Creating a Proxmox image template<br>\n    </h2>\n\n  <p>\n    One of the first steps is to create an image template that we can use to quickly spin up new virtual machines (VMs) based on cloud-image. This template will serve as our base image for the registry VM in our setup. I already documented the involved steps in my post <a href=\"https://norocketscience.at/about/\">\"Deploy Proxmox virtual machines using Cloud-init\"</a>\n  </p>\n\n    <h2 id=\"downloading-necessary-images\">\n      Downloading necessary images\n    </h2>\n\n  <p>\n    Next, you’ll need to download images for the following components:\n  </p>\n\n  <p>\n    Ubuntu Desktop: <a href=\"https://releases.ubuntu.com/24.04.3/ubuntu-24.04.3-desktop-amd64.iso\">Ubuntu Desktop 24.03.3 LTS Amd64</a><br>PFSense Firewall: <a href=\"https://atxfiles.netgate.com/mirror/downloads/pfSense-CE-2.7.2-RELEASE-amd64.iso.gz\">pfSense CE 2.7.2</a><br>\n  </p>\n\n  <p>\n    You can download it directly through the Proxmox GUI:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/proxmox1.jpg\" height=\"750\" width=\"1440\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/proxmox1-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/proxmox1-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/proxmox1-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/proxmox1-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/proxmox1-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/proxmox1-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/proxmox2.jpg\" height=\"586\" width=\"1196\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/proxmox2-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/proxmox2-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/proxmox2-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/proxmox2-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/proxmox2-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/proxmox2-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Copy the URL for the image into the \"URL\" field and click \"Query URL\" - after that you should see the file name which you put into the OpenTofu config in the next steps. Click on \"Download\" to download the file directly into Proxmox. Continue with all the components listed above.\n  </p>\n\n    <h2 id=\"configure-keyless-login-for-cloud-init\">\n      Configure keyless login for cloud-init\n    </h2>\n\n  <p>\n    You can always add a ssh key to your cloud-init configuration, but for now I'm activating password login for the ubuntu user in the ubuntu cloud images. For this you need to copy the following cloud-init configuration to /var/lib/vz/snippets (you have to create the snippets folder&nbsp;\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>#cloud-config\nusers:\n  - name: ubuntu\n    shell: /bin/bash\n    sudo: ['ALL=(ALL) NOPASSWD:ALL']\nssh_pwauth: True ## This line enables ssh password authentication</code></pre>\n\n    <h2 id=\"creating-proxmox-network-bridges\">\n      Creating Proxmox network bridges\n    </h2>\n\n  <p>\n    Proper network setup is crucial for implementing the lab. We’ll create multiple network bridges to isolate traffic between different components in conjunction wit pfSense. Remember from the first post we need three different networks:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/proxmox3-2.jpg\" height=\"277\" width=\"1310\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/proxmox3-2-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/proxmox3-2-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/proxmox3-2-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/proxmox3-2-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/proxmox3-2-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/proxmox3-2-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/proxmox4.jpg\" height=\"257\" width=\"602\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/proxmox4-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/proxmox4-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/proxmox4-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/proxmox4-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/proxmox4-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/proxmox4-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    These networks are local only Linux bridges. Means that they don't have any other network ports bridged and cannot communicate outside your Proxmox node. Just add a bridge, fill in the device name <em>vmbrX</em> and the CIDR notation for the network. In my example I used the following network convention:\n  </p>\n\n  <ul>\n    <li>DMZ - dmz.lab.internal 172.16.100.0/24</li><li>MGMT mgmt.lab.internal 192.168.100.0/24</li><li>SRV - srv.lab.internal 10.10.100.0/24</li>\n  </ul>\n\n  <p>\n    Additionally, we have the main network connection, needed for pfSense as <em>WAN</em> port. So all outgoing traffic will go through this one. Normally this is <em>vmbr0</em> in a default installation of Proxmox. For convenience reasons I've created an additional vlan in my home network to create a <em>CLIENT</em> network inside Proxmox to reach the management client (which will be installed later) directly from my home setup. It is possible to achieve this through the default lan connection too, but I didn't like the idea to mix up the functionality and to get a clear cut between INGRESS and EGRESS traffic.\n  </p>\n\n    <h2 id=\"provisioning-through-opentofu\">\n      Provisioning through OpenTofu\n    </h2>\n\n  <p>\n    As stated in the first post I'm going to use <a href=\"https://opentofu.org\">OpenTofu</a> for installation of the VMs. Everyone who uses Terraform should have not problem with OpenTofu which is the Open-Source fork. I'm not going into installation of the cli. This can be found in the <a href=\"https://opentofu.org/docs/intro/install/\">cli installation guide</a> for your environment.\n  </p>\n\n  <p>\n    For the lab the following infrastructure is needed:\n  </p>\n\n  <ul>\n    <li>1 x Management Client</li><li>1 x pfSense Firewall</li><li>1 x HAProxy Ingress</li><li>1 x Registry</li><li>1 x Talos control plane node</li><li>1 x Talos worker node</li>\n  </ul>\n\n  <p>\n    If you are new to Terraform provisioning with Proxmox I can recommend a blog post I've written years ago: <a href=\"https://norocketscience.at/install-proxmox-virtual-machines-with-terraform-2/\">Install Proxmox virtual machines with Terraform</a>&nbsp;The post will guide you through the configuration of the needed provider and describe the lifecycle of a typical provisioning process.\n  </p>\n\n  <p>\n    To speed up the deployment process I've already created a <a href=\"https://github.com/tbrandstetter/norocketscience-examples/tree/main/talos-kubernetes-cluster-part2\">ready to use configuration on in my GIT repository</a>. You just have to change the user/pwd of the api user as well as the Proxmox hostname in the provider config. If you have different image names created during the image download process or use different template names for your Proxmox templates you have to change that, as well as for changed vmbrX network names.\n  </p>\n\n  <p>\n    Be aware that the Talos control plane and the Talos worker node ist <strong>disabled for now</strong>. We're going to enable that and rerun OpenTofu after the Talos image factory has been installed and configured. This is necessary, because we're using SecureBoot and if the images change during installation, you have to reset the bios state.\n  </p>\n\n  <p>\n    Back to business. To start with OpenTofu three different yaml files have to be created:\n  </p>\n\n  <ul>\n    <li>provider.tf --&gt; holds the configuration for the Proxmox provider</li><li>variables.tf --&gt; replaces the placeholder in all other configuration files</li><li>main.tf --&gt; includes all the information about the VMs we're going to provision</li>\n  </ul>\n<pre class=\"line-numbers  language-json\"><code># provider.tf\n\nterraform {\n  required_providers {\n    proxmox = {\n      source  = \"telmate/proxmox\"\n      version = \"3.0.2-rc07\"\n    }\n  }\n}\n\nprovider \"proxmox\" {\n  pm_parallel     = 1\n  pm_tls_insecure = true\n  pm_api_url      = var.pm_api_url\n  pm_password     = var.pm_password\n  pm_user         = var.pm_user\n  pm_log_enable   = true\n  pm_log_file     = \"terraform-plugin-proxmox.log\"\n  pm_debug        = true\n  pm_log_levels = {\n    _default    = \"debug\"\n    _capturelog = \"\"\n  }\n}\n</code></pre>\n<pre class=\"line-numbers  language-json\"><code># variables.tf\n\nvariable \"pm_api_url\" {\n  default = \"https://fill in your hostname:8006/api2/json\"\n}\n\nvariable \"pm_user\" {\n  default = \"fill in your user\"\n}\n\nvariable \"pm_password\" {\n  default = \"fill in your password\"\n}\n</code></pre>\n<pre class=\"line-numbers  language-json\"><code># main.tf\n\n# Control Plane\n# resource \"proxmox_vm_qemu\" \"controlplane\" {\n#   name        = \"talos-cp-${count.index + 1}\"\n#   target_node = \"proxmox2\"\n#   count       = 1\n#   bios        = \"ovmf\"\n#   agent       = 1\n#   machine     = \"q35\"\n#   skip_ipv6   = true\n\n#   startup_shutdown {\n#     order             = 3\n#     shutdown_timeout  = -1\n#     startup_delay     = -1\n#   }\n\n#   start_at_node_boot = true\n#   cpu {\n#     cores = 4\n#     sockets = 1\n#     type = \"host\"\n#   }\n#   memory   = 6144\n#   scsihw   = \"virtio-scsi-pci\"\n#   boot     = \"order=scsi0;ide2\"\n\n#   disks {\n#     ide {\n#       ide2 {\n#         cdrom {\n#           iso = \"local:iso/metal-amd64-secureboot.iso\"\n#         }\n#       }\n#     }\n#     scsi {\n#       scsi0 {\n#         disk {\n#           size     = \"64G\"\n#           storage  = \"data\"\n#         }\n#       }\n#     }\n#   }\n\n#   efidisk {\n#     efitype = \"4m\"\n#     storage = \"data\"\n#   }\n\n#   network {\n#     model  = \"virtio\"\n#     bridge = \"vmbr3\"\n#     firewall = false\n#     link_down = false\n#     id = 1\n#   }\n\n#   rng {\n#     period = 0\n#     source = \"/dev/urandom\"\n#   }\n\n#   vga {\n#     type   = \"std\"\n#   }\n# }\n\n# Worker\n# resource \"proxmox_vm_qemu\" \"worker\" {\n#   name        = \"talos-worker-${count.index +1}\"\n#   target_node = \"proxmox2\"\n#   count       = 1\n#   bios        = \"ovmf\"\n#   agent       = 1\n#   machine     = \"q35\"\n#   skip_ipv6   = true\n\n#   startup_shutdown {\n#     order             = 4\n#     shutdown_timeout  = -1\n#     startup_delay     = -1\n#   }\n\n#   start_at_node_boot = true\n#   cpu {\n#     cores = 4\n#     sockets = 1\n#     type = \"host\"\n#   }\n#   memory   = 8172\n#   scsihw   = \"virtio-scsi-pci\"\n#   boot     = \"order=scsi0;ide2\"\n\n#   disks {\n#     ide {\n#       ide2 {\n#         cdrom {\n#           iso = \"local:iso/metal-amd64-secureboot.iso\"\n#         }\n#       }\n#     }\n#     scsi {\n#       scsi0 {\n#         disk {\n#           size     = \"100G\"\n#           storage  = \"data\"\n#         }\n#       }\n#     }\n#   }\n\n#   efidisk {\n#     efitype = \"4m\"\n#     storage = \"data\"\n#   }\n\n#   network {\n#     model  = \"virtio\"\n#     bridge = \"vmbr3\"\n#     firewall = false\n#     link_down = false\n#     id = 1\n#   }\n\n#   rng {\n#     period = 0\n#     source = \"/dev/urandom\"\n#   }\n\n#   vga {\n#     type   = \"std\"\n#   }\n# }\n\n# PFSense Firewall\nresource \"proxmox_vm_qemu\" \"firewall\" {\n  name        = \"pfsense-${count.index +1}\"\n  target_node = \"proxmox2\"\n  count       = 1\n  bios        = \"ovmf\"\n  machine     = \"q35\"\n  skip_ipv6   = true\n\n  startup_shutdown {\n    order             = 1\n    shutdown_timeout  = -1\n    startup_delay     = -1\n  }\n\n  start_at_node_boot = true\n  cpu {\n    cores = 1\n    sockets = 1\n    type = \"host\"\n  }\n  memory   = 1024\n  boot     = \"order=virtio0;ide2\"\n\n  disks {\n    ide {\n      ide2 {\n        cdrom {\n          iso = \"local:iso/pfSense-CE-2.7.2-RELEASE-amd64.iso\"\n        }\n      }\n    }\n    virtio {\n      virtio0 {\n        disk {\n          size     = \"10G\"\n          storage  = \"data\"\n          iothread = true\n        }\n      }\n    }\n  }\n\n  efidisk {\n    efitype = \"4m\"\n    storage = \"data\"\n  }\n\n  # WAN\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr0\"\n    firewall = false\n    link_down = false\n    id = 0\n  }\n\n  # MGMT\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr1\"\n    firewall = false\n    link_down = false\n    id = 1\n  }\n\n  # DMZ\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr2\"\n    firewall = false\n    link_down = false\n    id = 2\n  }\n\n  # SRV\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr3\"\n    firewall = false\n    link_down = false\n    id = 3\n  }\n\n  rng {\n    period = 0\n    source = \"/dev/urandom\"\n  }\n\n  vga {\n    type   = \"std\"\n  }\n}\n\n# Bastion Host\nresource \"proxmox_vm_qemu\" \"bastion\" {\n  name        = \"bastion\"\n  target_node = \"proxmox2\"\n  count       = 1\n\n  clone = \"ubuntu-24.04-cloud-init-template\"\n\n  startup_shutdown {\n    order             = 2\n    shutdown_timeout  = -1\n    startup_delay     = -1\n  }\n\n  os_type  = \"cloud-init\"\n  start_at_node_boot = true\n  cpu {\n    cores = 2\n    sockets = 1\n    type = \"host\"\n  }\n  memory   = 4096\n  scsihw   = \"virtio-scsi-single\"\n  boot     = \"order=scsi0\"\n\n  disks {\n    ide {\n      ide0 {\n        cloudinit {\n          storage = \"data\"\n        }\n      }\n    }\n    scsi {\n      scsi0 {\n        disk {\n          size     = \"50G\"\n          storage  = \"data\"\n          iothread = true\n        }\n      }\n    }\n  }\n\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr1\"\n    id = 0\n  }\n\n  lifecycle {\n    ignore_changes = [\n      network,\n    ]\n  }\n\n  # Cloud Init Settings\n  ipconfig0 = \"ip=192.168.100.11/24,gw=192.168.100.1\"\n  nameserver = \"192.168.100.1\"\n  searchdomain = \"mgmt.lab.internal\"\n  cipassword  = \"ubuntu\"\n  cicustom    = \"vendor=local:snippets/ci-custom.yml\"\n  ciupgrade   = true \n}\n\n# HAProxy Ingress Host\nresource \"proxmox_vm_qemu\" \"haproxy-ingess\" {\n  name        = \"haproxy-ingress-1\"\n  target_node = \"proxmox2\"\n  count       = 1\n\n  clone = \"ubuntu-24.04-cloud-init-template\"\n\n  startup_shutdown {\n    order             = 5\n    shutdown_timeout  = -1\n    startup_delay     = -1\n  }\n\n  os_type  = \"cloud-init\"\n  start_at_node_boot = true\n  cpu {\n    cores = 2\n    sockets = 1\n    type = \"host\"\n  }\n  memory   = 4096\n  scsihw   = \"virtio-scsi-single\"\n  boot     = \"order=scsi0\"\n\n  disks {\n    ide {\n      ide0 {\n        cloudinit {\n          storage = \"data\"\n        }\n      }\n    }\n    scsi {\n      scsi0 {\n        disk {\n          size     = \"50G\"\n          storage  = \"data\"\n          iothread = true\n        }\n      }\n    }\n  }\n\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr2\"\n    id = 0\n  }\n\n  lifecycle {\n    ignore_changes = [\n      network,\n    ]\n  }\n\n  # Cloud Init Settings\n  ipconfig0 = \"ip=172.16.100.10/24,gw=172.16.100.1\"\n  nameserver = \"172.16.100.1\"\n  searchdomain = \"srv.lab.internal\"\n  cipassword  = \"ubuntu\"\n  cicustom    = \"vendor=local:snippets/ci-custom.yml\"\n  ciupgrade   = true \n}\n\n\n# Management Client\nresource \"proxmox_vm_qemu\" \"mgmt-client\" {\n  name        = \"mgmt-client\"\n  target_node = \"proxmox2\"\n  count       = 1\n  bios        = \"ovmf\"\n  agent       = 1\n  machine     = \"q35\"\n  skip_ipv6   = true\n\n  startup_shutdown {\n    order             = 6\n    shutdown_timeout  = -1\n    startup_delay     = -1\n  }\n\n  start_at_node_boot = true\n  cpu {\n    cores = 2\n    sockets = 1\n    type = \"host\"\n  }\n  memory   = 4096\n  scsihw   = \"virtio-scsi-pci\"\n  boot     = \"order=scsi0;ide2\"\n\n  disks {\n    ide {\n      ide2 {\n        cdrom {\n          iso = \"local:iso/ubuntu-24.04.3-desktop-amd64.iso\"\n        }\n      }\n    }\n    scsi {\n      scsi0 {\n        disk {\n          size     = \"64G\"\n          storage  = \"data\"\n        }\n      }\n    }\n  }\n\n  efidisk {\n    efitype = \"4m\"\n    storage = \"data\"\n  }\n\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr1\"\n    firewall = false\n    link_down = false\n    id = 1\n  }\n\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr4\"\n    firewall = false\n    link_down = false\n    id = 2\n  }\n\n  rng {\n    period = 0\n    source = \"/dev/urandom\"\n  }\n\n  vga {\n    type   = \"virtio\"\n    memory = 128\n  }\n}</code></pre>\n\n  <p>\n    After the files have ben set up, the provider needs to be installed first:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>tofu init\n</code></pre>\n\n  <p>\n    If everything wents fine you should see the following information:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/tofu1.jpg\" height=\"560\" width=\"1938\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/tofu1-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/tofu1-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/tofu1-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/tofu1-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/tofu1-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/tofu1-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Now we can plan our provisioning:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>tofu plan</code></pre>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/tofu2.jpg\" height=\"264\" width=\"654\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/tofu2-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/tofu2-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/tofu2-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/tofu2-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/tofu2-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/tofu2-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    There should be only add's for the first run. Last but not least tofu apply will start the provisioning:\n  </p>\n<pre class=\"line-numbers  language-html\"><code>tofu apply</code></pre>\n\n  <p>\n    In the end you should see the new installed VMs in your Proxmox instance (Talos nodes excluded right now!)\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/proxmox6.jpg\" height=\"598\" width=\"444\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/proxmox6-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/proxmox6-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/proxmox6-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/proxmox6-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/proxmox6-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/proxmox6-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <h2 id=\"install-ubuntu-management-client\">\n      Install Ubuntu management client\n    </h2>\n\n  <p>\n    For managing pfSense and as a passthrough for or other VMs I'm using Ubuntu Desktop. I'm not going into detail how to install an Linux desktop system, because on this level we're operating it should be a no brainer. If you need a guidance, there is a great tutorial on&nbsp;<a href=\"https://thedev.uk/how-to-install-ubuntu-desktop-24-04-1-on-proxmox-a-comprehensive-guide/\">thedev.uk</a>\n  </p>\n\n  <p>\n    The key points for the installation are:\n  </p>\n\n  <ul>\n    <li>&nbsp;Use just the Basic Software</li><li>Install 3rd Party Software</li><li>(Optional) Disable Password for Login</li><li>apt install qemu-guest agent ssh<br></li>\n  </ul>\n\n  <p>\n    Additionally, we disabling the automatic gateway setting in the second network interface which sould be the lan/vlan to your homelab client network:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/network1-2.jpg\" height=\"614\" width=\"1956\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/network1-2-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/network1-2-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/network1-2-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/network1-2-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/network1-2-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/network1-2-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/network2.jpg\" height=\"968\" width=\"1502\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/network2-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/network2-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/network2-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/network2-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/network2-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/network2-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    This is needed to avoid asynchronous routing between your network outside of Proxmox and pfSense.\n  </p>\n\n    <h2 id=\"configure-pfsense\">\n      Configure pfSense\n    </h2>\n\n  <p>\n    Go to the Proxmox console of your pfSense host and start the installation. You can use the standard values in the installation setup. For the disk filesystem I personally prefer UFS for this lab.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense7.jpg\" height=\"1058\" width=\"1697\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense7-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense7-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense7-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense7-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense7-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense7-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense8.jpg\" height=\"1058\" width=\"1697\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense8-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense8-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense8-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense8-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense8-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense8-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense9.jpg\" height=\"1058\" width=\"1697\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense9-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense9-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense9-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense9-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense9-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense9-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense10.jpg\" height=\"1058\" width=\"1697\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense10-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense10-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense10-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense10-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense10-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense10-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense11.jpg\" height=\"1058\" width=\"1697\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense11-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense11-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense11-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense11-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense11-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense11-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    After a reboot we could enter the initial WAN and MGMT interface configuration so that we can do the rest of the setup over the web browser:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense.jpg\" height=\"1014\" width=\"2332\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Continue this for vtnet2 and vtnet3. The network interfaces in the console should look like this right now\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense13.jpg\" height=\"365\" width=\"1697\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense13-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense13-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense13-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense13-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense13-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense13-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <ul>\n    <li>WAN -&gt; vtnet0 -&gt; v4/DHCP4 -&gt; Server lan ip</li><li>LAN -&gt; vtnet1 -&gt; v4</li><li>OPT1 -&gt; vtnet2 -&gt; v4</li><li>OPT2 -&gt; vtnet3 -&gt; v4</li>\n  </ul>\n\n  <p>\n    To get access to the management gui from the management client, we need to statically set an ip address. For this lab we'll take the ip 192.168.100.1:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense2.jpg\" height=\"582\" width=\"2110\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense2-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense2-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense2-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense2-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense2-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense2-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense3.jpg\" height=\"384\" width=\"1766\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense3-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense3-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense3-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense3-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense3-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense3-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense17.jpg\" height=\"954\" width=\"1697\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense17-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense17-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense17-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense17-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense17-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense17-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Now you should be able to see the admin interface through the web browser on the management client\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense4.jpg\" height=\"1286\" width=\"2596\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense4-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense4-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense4-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense4-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense4-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense4-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    After typing in admin/pfsense you'll be guided through an initial setup process. On the general information screen (step 2 of 9) enter the following information:\n  </p>\n\n  <ul>\n    <li>hostname: firewall</li><li>domain: mgmt.lab.internal</li><li>dns: \"your internal dns\" or 8.8.8.8</li>\n  </ul>\n\n  <p>\n    On step 4 of 9 (Configure WAN interface) disable the private network block:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense5.jpg\" height=\"266\" width=\"2300\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense5-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense5-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense5-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense5-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense5-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense5-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    For the rest of the steps the default settings are ok. Before we're configuring the rest of the system update pfSense to the latest version if available. You can do that under System/Update.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/8/pfsense18.jpg\" height=\"891\" width=\"1851\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/8/responsive/pfsense18-xs.jpg 640w ,https://norocketscience.at/media/posts/8/responsive/pfsense18-sm.jpg 768w ,https://norocketscience.at/media/posts/8/responsive/pfsense18-md.jpg 1024w ,https://norocketscience.at/media/posts/8/responsive/pfsense18-lg.jpg 1366w ,https://norocketscience.at/media/posts/8/responsive/pfsense18-xl.jpg 1600w ,https://norocketscience.at/media/posts/8/responsive/pfsense18-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Now that we're finished with the initial configuration. It's time to configure the rest of the system so that we're get our lab to work. For this, I've already prepared a ready to use backup file. To install it go to <em>Diagnostics / Backup &amp; Restore</em> and upload the backup file&nbsp;from the GIT repository.\n  </p>\n\n  <p>\n    A list of the config change are:\n  </p>\n\n  <ul>\n    <li>Proper naming and configuration of all network interfaces</li><li>Activated Kea DHCP server for the lan interfaces</li><li>Activated DNS resolver with static entries for Talos</li><li>Installed and configured FRR package for the BGP config</li><li>FW rules for the whole lab</li>\n  </ul>\n\n  <p>\n    <strong>Please be aware, you have to change the MAC addresses for the static dhcp entries!</strong>\n  </p>\n\n  <p>\n    If everything went fine you should now be able to ssh into the Registry VM through the ip 192.168.100.11 which we need for the next part to install the internal image registry as well as the Talos Image Factory.\n  </p>\n\n    <h2 id=\"summary\">\n      Summary\n    </h2>\n\n  <p>\n    At the end of this post all VMs (except for the Talos nodes) should be up and running now. We have a management client set up which is reachable from outside Proxmox and can reach the pfSense firewall which is configured too. The registry VM which is in another network can be reached from the management client through ssh.\n  </p>\n\n  <p>\n    That's all for now. Stay tuned and leave me a <a href=\"https://tinyurl.com/4atbveyr\">comment on LinkedIn</a> for any problems or suggestions you have.\n  </p>\n\n  <p>\n    Related posts:\n  </p>\n\n  <p>\n    <a href=\"https://norocketscience.at/supercharged-air-gapped-talos-kubernetes-cluster-an-overview/\">Highly separated Talos Kubernetes Cluster: Part1 - Overview</a>\n  </p>",
            "image": "https://norocketscience.at/media/posts/8/geranimo-OomNPPv1Rpk-unsplash-2xl.jpg",
            "author": {
                "name": "Thomas Brandstetter"
            },
            "tags": [
            ],
            "date_published": "2026-01-30T12:26:32+01:00",
            "date_modified": "2026-02-05T08:38:12+01:00"
        },
        {
            "id": "https://norocketscience.at/supercharged-air-gapped-talos-kubernetes-cluster-an-overview/",
            "url": "https://norocketscience.at/supercharged-air-gapped-talos-kubernetes-cluster-an-overview/",
            "title": "Highly separated Talos Kubernetes Cluster: Part 1 - Overview",
            "summary": "Welcome to the overview of a series on building an Talos Kubernetes cluster with enhanced security. This journey will take you through every step, from setting up the initial infrastructure to deploying a fully functional demo application. In this introductory post, we'll discuss why I'm&hellip;",
            "content_html": "\n  <p>\n    Welcome to the overview of a series on building an Talos Kubernetes cluster with enhanced security. This journey will take you through every step, from setting up the initial infrastructure to deploying a fully functional demo application. In this introductory post, we'll discuss why I'm undertaking this project, the benefits it offers, and outline the developed architecture as well as necessary software and hardware components.\n  </p>\n\n    <h2 id=\"highly-separated-not-air-gapped\">\n      Highly separated not air-gapped\n    </h2>\n\n    <figure class=\"blockquote\">\n      <blockquote>An air gap is a security measure that involves isolating a computer or network and preventing it from establishing an external connection. An air-gapped computer is physically segregated and incapable of connecting wirelessly or physically with other computers or network devices.</blockquote>\n      <figcaption>https://www.techtarget.com/whatis/definition/air-gapping</figcaption>\n    </figure>\n\n  <p id=\"why-am-i-doing-it\">\n    In the last years the term \"air-gapped\" has been mistakenly used for systems who tried to minimise the attack vector through eliminating the direct connection to the internet.\n  </p>\n\n    <figure class=\"blockquote\">\n      <blockquote>In the modern world, almost nothing is air gapped in the original sense of the word, except for maybe some Safety Instrumented Systems, and the occasional CPU on a power drill, saw or other power tool. Security practitioners who use the word “air gap” to describe their connected networks are sowing confusion. Stop it.</blockquote>\n      <figcaption>https://waterfall-security.com/ot-insights-center/ot-cybersecurity-insights-center/air-gaps-i-do-not-think-it-means-what-you-think-it-means/</figcaption>\n    </figure>\n\n  <p>\n    So, I'm going to use the term \"highly separated\" for the rest of this blog series to reflect an enhanced security pattern used for this architecture.\n  </p>\n\n    <h2 id=\"why-am-i-doing-it\">\n      Why am I doing it?\n    </h2>\n\n  <p>\n    In today's rapidly evolving IT landscape, security and isolation are paramount. A Kubernetes cluster within a dedicated network and internal set up registries provides an additional layer of security by moving the services away from external access points. This setup is particularly useful for organisations handling sensitive data or operating in highly regulated industries.\n  </p>\n\n    <h2 id=\"what-are-the-benefits\">\n      What are the benefits?\n    </h2>\n\n  <p>\n    <u>Enhanced Security:</u> By using strict rules to move the cluster away from other networks, we reduce potential attack vectors.\n  </p>\n\n  <p>\n    <u>Compliance:</u> Many regulations require strict separation of systems handling sensitive information.\n  </p>\n\n  <p>\n    <u>Performance:</u> Dedicated resources can lead to more predictable and consistent performance for applications running within the cluster.\n  </p>\n\n    <h2 id=\"what-am-i-going-to-do\">\n      What am I going to do?\n    </h2>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/7/Untitled-Diagram.drawio-7.png\" height=\"1041\" width=\"797\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-xs.png 640w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-sm.png 768w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-md.png 1024w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-lg.png 1366w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-xl.png 1600w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-2xl.png 1920w\">\n      \n    </figure>\n\n  <p>\n    So what do we see here? First of all three networks are defined in Proxmox to support the Kubernetes deployment in this lab:\n  </p>\n\n  <ul>\n    <li><u>DMZ</u> - houses the HAProxy and HAProxy Ingress Controller and is used for external traffic to our example application after successful installation</li><li><u>MGMT</u> - is used for redirect necessary registry traffic and Talos internal image factory functionality. It takes care that Talos has no need to connect to the internet to get the needed images. Furthermore, the management client is installed here to have a convenient place for installation, both through cli and browser gui</li><li><u>SRV</u> - is reserved for the TALOS cluster installation only.</li>\n  </ul>\n\n  <p>\n    All defined networks are secured by pfSense which takes care that only necessary and allowed traffic goes through. The detailed firewall config will be discussed in later parts of this series.\n  </p>\n\n  <p>\n    Out of scope is the network out of Proxmox. The assumption is that you will have two separated lan (vlan or else) definitions to work with. One client lan to get into the management client seamlessly and one server lan where the WAN traffic from pfSense goes through and ingress traffic flows in.\n  </p>\n\n  <p>\n    Based on my research the setup will involve the following software components.\n  </p>\n\n    <h3 id=\"proxmox-ve\">\n      Proxmox VE\n    </h3>\n\n  <p>\n    Proxmox VE is built on open-source principles, fostering a community-driven approach that ensures continuous security, improvement and innovation. This openness is a perfect fit for the setup I like to implement. Additionally,&nbsp;Proxmox VE offers comprehensive backup and replication solutions to safeguard your data against loss or corruption.\n  </p>\n\n    <h3 id=\"opentofu\">\n      OpenTofu\n    </h3>\n\n  <p>\n    For the VM proviosioning I had to decide between Terraform and OpenTofu. I've chosen OpenTofu because it is fully open-source, which means you have complete transparency into how it works. This can be crucial for security-conscious organizations. Furthermore it encourages contributions from a wider community, potentially leading to faster bug fixes and new features.\n  </p>\n\n    <h3 id=\"pfsense\">\n      pfSense\n    </h3>\n\n  <p>\n    pfSense is my preferred choice for separating and controlling network traffic in several ways:\n  </p>\n\n  <p>\n    <u>Network segmentation:</u>&nbsp;pfSense allows me to connect multiple, isolated network segments within a single physical network infrastructure in a virtualised environment like Proxmox. This helps prevent broadcasts between networks, reducing congestion and improving security.\n  </p>\n\n  <p>\n    <u>Firewall control:</u>&nbsp;With the integrated stateful firewall, I can define granular rules for traffic flow between these segments. I can allow or block specific ports/protocols, restrict access based on source/destination IP, and even shape bandwidth to prioritize critical services.\n  </p>\n\n  <p>\n    <u>Additional Services:</u>&nbsp;pfSense includes several additional features like dhcp dns resolver and BGP support. These help to bundle the needed functionality in one point.\n  </p>\n\n  <p>\n    <u>Centralized Management:</u>&nbsp;pfSense provides a web-based GUI for managing all these features from a central location. This makes it easier to maintain consistency across multiple networks.\n  </p>\n\n  <p>\n    <u>Cost-Effective: </u>Since the firewall is open-source, I can run it on affordable hardware or virtual machines, making it a cost-effective solution compared to commercial alternatives.\n  </p>\n\n  <p>\n    In essence, pfSense gives me the tools and control I need to design this secure, efficient network architecture tailored to my specific requirements.\n  </p>\n\n    <h3 id=\"ubuntu\">\n      Ubuntu\n    </h3>\n\n  <p>\n    Not much to say here. It is my preferred Linux distribution to work with. Im going to use it as a Management client to manage the entire setup from one host and it is additionally used to set up the bastion host (Harbor and HAProxy).\n  </p>\n\n    <h3 id=\"harbor\">\n      Harbor\n    </h3>\n\n  <p>\n    Harbor is an open-source cloud-native registry that stores, signs, and scans container images for vulnerabilities. It provides advanced features like content signing, image replication, and role-based access control (RBAC). With Harbor, I can securely store Docker and OCI images and ensure their integrity through cryptographic signatures. Furthermore, it is possible to activate vulnerability scanning on uploaded images. These security aspects are crucial to complete the air gapped architecture for Talos. The integrated web portal offers a user-friendly interface for managing repositories, while the included API enables integration for future use CI/CD pipelines.&nbsp;\n  </p>\n\n    <h3 id=\"talos\">\n      Talos\n    </h3>\n\n  <p>\n    Coming from OpenShift in my professional work environment I was stunned by Talos regarding security and simplicity. I decided to go for Talos because of:\n  </p>\n\n  <ol>\n    <li><u>Minimal attack surface:</u> Talos is a minimal, immutable operating system that only includes essential components for running Kubernetes. This reduced attack surface minimises potential vulnerabilities and makes the system easier to secure.</li><li><span style=\"font-size: 1em;\"><u>Immutable infrastructure:</u> Since Talos uses an immutable infrastructure model, changes are made through atomic updates, ensuring consistency and reducing the risk of configuration drift or human error introducing security flaws.</span></li><li><u>Simplified security updates:</u> Talos simplifies the process of patching and updating the system, which means critical security patches can be applied quickly without complex installation steps, keeping the cluster nodes secure against known vulnerabilities.</li><li><u>Strong default security posture:</u> Out of the box, Talos enforces strong security practices like read-only root filesystems, mandatory use of cryptographic signatures for updates, and strict security policies. This minimises misconfigurations that could lead to security issues.</li>\n  </ol>\n\n    <h3 id=\"haproxy\">\n      HAProxy\n    </h3>\n\n  <p>\n    HAProxy is renowned for its high performance, reliability, and low resource consumption. It's designed to handle a large number of connections efficiently, making it ideal for managing incoming traffic at scale. Its advanced load balancing algorithms (round-robin, least connections, etc.) in combination with the build-in SSL/TLS support allows us to distribute traffic intelligently across backend services, ensuring optimal use of resources and improved application performance. On top of that it delivers enhanced security services like allow-/denylisting, rate limiting and DDoS protection.\n  </p>\n\n  <p>\n    HAProxy as an Ingress controller integrates seamlessly with Kubernetes, enabling dynamic updates based on Kubernetes Ingress resources. This allows for easy management of application routes directly within the Kubernetes ecosystem and makes this component a vital part in the cloud native architecture.\n  </p>\n\n    <h3 id=\"calico\">\n      Calico\n    </h3>\n\n  <p>\n    Calico is an open-source networking solution designed to provide low-latency, high-performance network connectivity for containers by leveraging the Linux kernel for routing packages.  Unlike some CNI solutions that rely on encapsulation (like VXLAN or IPIP), Calico can use pure Layer 3 routing. This reduces overhead and allows for better performance, especially in large-scale environments. The final and most important part is BGP. By integrating BGP (Border Gateway Protocol), Calico enables efficient routing of traffic across different networks, which is essential for the planned architecture.In the end it will be a perfect fit to the HAProxy counterpart.\n  </p>\n\n    <h2 id=\"next-steps\">\n      Next steps\n    </h2>\n\n  <p>\n    Now that we nailed down the architecture and the used components, it is time to prepare the most important part of this setup.\n  </p>\n\n  <p>\n    To get started, you'll need:<br>\n  </p>\n\n  <ul>\n    <li>Proxmox VE installed on a physical server or virtual machine</li><li>Internet access to the necessary ISO images and software packages.</li><li>A reliable network setup with appropriate subnets and/or VLANs.</li>\n  </ul>\n\n  <p>\n    In subsequent posts over the next weeks, I will delve into each component in detail. Stay tuned for our next post, where we'll guide you through preparing your Proxmox environment!\n  </p>\n\n  <p class=\"msg msg--info\">\n    Being a non native English speaker, some parts of this text have been written with the support of artificial intelligence.\n  </p>\n\n  <p>\n    Discuss or comment this post on <a href=\"https://tinyurl.com/4atbveyr\">LinkedIn</a>\n  </p>\n\n  <p>\n    Related posts:\n  </p>\n\n  <p>\n    <a href=\"https://norocketscience.at/highly-separated-talos-kubernetes-cluster-part2-vm-installation/\">Highly separated Talos Kubernetes Cluster: Part 2 - VM installation</a>\n  </p>",
            "image": "https://norocketscience.at/media/posts/7/geranimo-OomNPPv1Rpk-unsplash.jpg",
            "author": {
                "name": "Thomas Brandstetter"
            },
            "tags": [
            ],
            "date_published": "2026-01-29T20:46:00+01:00",
            "date_modified": "2026-02-04T07:44:39+01:00"
        },
        {
            "id": "https://norocketscience.at/install-proxmox-virtual-machines-with-terraform-2/",
            "url": "https://norocketscience.at/install-proxmox-virtual-machines-with-terraform-2/",
            "title": "Install Proxmox virtual machines with Terraform",
            "summary": "A few words about Terraform If you have read my last blog post you already know how to create templates for further provisioning using the Cloud-init specification. In this post I like to show you how you easily deploy your infrastructure using Terraform on the&hellip;",
            "content_html": "\n    <h2 id=\"a-few-words-about-terraform\">\n      A few words about Terraform\n    </h2>\n\n  <p>\n    If you have read my <a href=\"https://norocketscience.at/about/\">last blog post</a> you already know how to create templates for further provisioning using the Cloud-init specification. In this post I like to show you how you easily deploy your infrastructure using Terraform on the virtualisation solution Proxmox. First of all, what is Terraform exactly? Terraform is a so called \"Infrastructure as a code\" software developed by <a href=\"https://www.hashicorp.com\">Hashicorp</a> - a company which also created <a href=\"https://www.vagrantup.com\">Vagrant</a> and other famous tools for professional cloud solutions. According to <a href=\"https://docs.microsoft.com/en-us/azure/devops/learn/what-is-infrastructure-as-code\">Microsoft</a>,<br>\n  </p>\n\n    <blockquote class=\"blockquote\">\n      Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code.<br>\n    </blockquote>\n\n  <p>\n    What's the benefit you may ask. One of the biggest advantages is that you don't have to install your servers and other components manually. Instead, with just a few lines of code you're able to manage your complete infrastructure. This automation step brings your stability, quality improvements, resilience and time for more important things. Furthermore, you have the possibility to integrate your Terraform configuration into your existing VCS and automatically run changes through your infrastructure with a proper CI/CD pipeline.<br>\n  </p>\n\n  <p>\n    Another good question - why use Terraform if you've already have Ansible in place?Have you ever tried to remove a package in Ansible? If you just remove it from you playbook configuration it is still on your server until you write the right configuration (the attribute \"absent\" is your friend) or remove it manualy. Terraform takes care on this changes and has a clever dependency management for \"First this, then that\" decisions. Writing about the differences between the two would hijack my \"Terraform practicing\" article, so to keep a long story short:\n  </p>\n\n  <ul>\n    <li>Use Terraform for all your infrastructure stuff (VMs and the baseline software on it)</li><li>Use Ansible for the configuration management (Installation of Applications and the configuration of it)</li>\n  </ul>\n\n    <h2 id=\"lets-start-practicing\">\n      Let's start practicing\n    </h2>\n\n    <h3 id=\"assumptions\">\n      Assumptions\n    </h3>\n\n  <ul>\n    <li>You have already a Proxmox server installed (it works with other virtualisation and cloud providers too - but this article is about Proxmox and Terraform)</li><li>You have already OS-templates with Cloud-init defined (if not, please read my <a href=\"https://norocketscience.at/about/\">blog post</a> about it)</li>\n  </ul>\n\n    <h3 id=\"installation-of-terraform\">\n      Installation of Terraform\n    </h3>\n\n  <p>\n    You can install Terraform on the major plattforms using either the package manager/appstore from the chosen system or download the binaries from <br><a href=\"https://developer.hashicorp.com/terraform/install\">Hashicorp</a> directly. On OSX I used <a href=\"https://brew.sh/\">Homebrew</a>, which is a package manager for OSX.\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>bash\nbrew install terraform</code></pre>\n\n    <h3 id=\"configure-your-first-terraform-project\">\n      Configure your first Terraform project\n    </h3>\n\n  <p>\n    Now we're going to configure our first Terraform infrastructure which runs on Proxmox. In my example, I like to install a small Kubernetes infrastructure which should run <a href=\"https://k3s.io/\">K3s (a lightweight Kubernetes distribution)</a> later on. For that <br>we have to define the following environment:<br>\n  </p>\n\n  <ul>\n    <li>1 x Master Server</li><li>2 x Node Server</li><li>1 x Storage Server (for persistent storage)</li>\n  </ul>\n\n    <h4 id=\"create-the-proxmox-api-user\">\n      Create the Proxmox api user\n    </h4>\n\n  <p>\n    A prerequisite for using the Proxmox provider is a working user with the right permissions in Proxmox. Go to the Datacenter/Permissions/User settings in Proxmox and add a user. Make sure to use the <u>Proxmox VE Authentication server setting</u>:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/5/proxmox.jpg\" height=\"388\" width=\"754\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/5/responsive/proxmox-xs.jpg 640w ,https://norocketscience.at/media/posts/5/responsive/proxmox-sm.jpg 768w ,https://norocketscience.at/media/posts/5/responsive/proxmox-md.jpg 1024w ,https://norocketscience.at/media/posts/5/responsive/proxmox-lg.jpg 1366w ,https://norocketscience.at/media/posts/5/responsive/proxmox-xl.jpg 1600w ,https://norocketscience.at/media/posts/5/responsive/proxmox-2xl.jpg 1920w\">\n      \n    </figure>\n\n  <p>\n    Under Datacenter/Permissions/Roles you can assign the necessary roles for the specific user. The actual permission list can be found on the <a href=\"https://search.opentofu.org/provider/telmate/proxmox/latest\">provider</a> side too:\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://norocketscience.at/media/posts/5/proxmox5.jpg\" height=\"197\" width=\"2330\" alt=\"\"  sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://norocketscience.at/media/posts/5/responsive/proxmox5-xs.jpg 640w ,https://norocketscience.at/media/posts/5/responsive/proxmox5-sm.jpg 768w ,https://norocketscience.at/media/posts/5/responsive/proxmox5-md.jpg 1024w ,https://norocketscience.at/media/posts/5/responsive/proxmox5-lg.jpg 1366w ,https://norocketscience.at/media/posts/5/responsive/proxmox5-xl.jpg 1600w ,https://norocketscience.at/media/posts/5/responsive/proxmox5-2xl.jpg 1920w\">\n      \n    </figure>\n\n    <h4 id=\"configure-the-proxmox-provider\">\n      Configure the Proxmox provider\n    </h4>\n\n  <p>\n    First, we configure the connection settings for the Proxmox provider. I chose the provider from <a href=\"https://github.com/Telmate/terraform-provider-proxmox\">Telmate</a> for the Proxmox provisioning. For better readability of our infrastructure code, we split variables and provider in two different configuration files named <em></em><strong><em>variables.tf</em> </strong>and <strong><em>provider.tf</em></strong>\n  </p>\n<pre class=\"line-numbers  language-json\"><code># variables.tf\n\nvariable \"pm_api_url\" {\n  default = \"https://your-ip:8006/api2/json\"\n}\n\nvariable \"pm_user\" {\n  default = \"terraform-prov@pve\"\n}\n\nvariable \"pm_password\" {\n  default = \"your-password\"\n}\n</code></pre>\n<pre class=\"line-numbers  language-json\"><code># provider.tf\n\nterraform {\n  required_providers {\n    proxmox = {\n      source  = \"telmate/proxmox\"\n      version = \"3.0.2-rc07\"\n    }\n  }\n}\n\nprovider \"proxmox\" {\n  pm_parallel     = 1\n  pm_tls_insecure = true\n  pm_api_url      = var.pm_api_url\n  pm_password     = var.pm_password\n  pm_user         = var.pm_user\n  pm_log_enable   = true\n  pm_log_file     = \"terraform-plugin-proxmox.log\"\n  pm_debug        = true\n  pm_log_levels = {\n    _default    = \"debug\"\n    _capturelog = \"\"\n  }\n}\n</code></pre>\n\n    <h4 id=\"configure-the-virtual-machines\">\n      Configure the virtual machines\n    </h4>\n\n  <p>\n    Next step is the main configuration of our k3s-cluster server. Here you have to adapt the following attributes according to your configuration:\n  </p>\n\n  <ul>\n    <li>target_node (the name of your Proxmox instance)</li><li>name (the name of the virtual server)</li><li>clone (the name of the template in Proxmox)</li><li>cores</li><li>memory</li><li>storage (the right storage pool in Proxmox)</li><li>ipconfig0 (Use the right IP range for your servers - the count.index is necessary if you have more then one server configured - like the k3s_agents in the example below)</li>\n  </ul>\n\n  <p>\n    The \"ignore changes\" lifecycle block is necessary, because Terraform likes to change the mac address on the second run - maybe a problem in the Proxmox provider - see here: <a href=\"https://github.com/Telmate/terraform-provider-proxmox/issues/112\">https://github.com/Telmate/terraform-provider-proxmox/issues/112</a>\n  </p>\n<pre class=\"line-numbers  language-json\"><code># main.cf\n\nresource \"proxmox_vm_qemu\" \"k3s_server\" {\n  count             = 1\n  name              = \"kubernetes-master-${count.index}\"\n  target_node       = \"homelab\"\n\n  clone             = \"ubuntu-2404-cloudinit-template\"\n\n  os_type           = \"cloud-init\"\n  cpu {\n    cores             = 4\n    sockets           = \"1\"\n    type               = \"host\"\n  }\n  memory            = 1024\n  scsihw            = \"virtio-scsi-pci\"\n  boot              = \"order=scsi0\"\n\n  disks {\n    scsi {\n      scsi0 {\n        disk {\n          size     = \"20G\"\n          storage  = \"data\"\n        }\n      }\n    }\n  }\n\n  network {\n    id              = 0\n    model           = \"virtio\"\n    bridge          = \"vmbr0\"\n  }\n\n  lifecycle {\n    ignore_changes  = [\n      network,\n    ]\n  }\n\n  # Cloud Init Settings\n  ipconfig0         = \"ip=192.168.2.11${count.index + 1}/24,gw=192.168.2.1\"\n\n  sshkeys = &lt;&lt;EOF\n  ${var.ssh_key}\n  EOF\n}\n\nresource \"proxmox_vm_qemu\" \"k3s_agent\" {\n  count             = 2\n  name              = \"kubernetes-node-${count.index}\"\n  target_node       = \"homelab\"\n\n  clone             = \"ubuntu-2404-cloudinit-template\"\n\n  os_type           = \"cloud-init\"\n  cpu {\n    cores             = 4\n    sockets           = \"1\"\n    type               = \"host\"\n  }\n  memory            = 1024\n  scsihw            = \"virtio-scsi-pci\"\n  boot              = \"order=scsi0\"\n\n  disks {\n    scsi {\n      scsi0 {\n        disk {\n          size     = \"20G\"\n          storage  = \"data\"\n        }\n      }\n    }\n  }\n\n  network {\n    id              = 0\n    model           = \"virtio\"\n    bridge          = \"vmbr0\"\n  }\n\n  lifecycle {\n    ignore_changes  = [\n      network,\n    ]\n  }\n\n  # Cloud Init Settings\n  ipconfig0         = \"ip=192.168.2.12${count.index + 1}/24,gw=192.168.2.1\"\n\n  sshkeys = &lt;&lt;EOF\n  ${var.ssh_key}\n  EOF\n}\n\nresource \"proxmox_vm_qemu\" \"storage\" {\n  count             = 1\n  name              = \"storage-node-${count.index}\"\n  target_node       = \"homelab\"\n\n  clone             = \"ubuntu-2404-cloudinit-template\"\n\n  os_type           = \"cloud-init\"\n  cpu {\n    cores             = 4\n    sockets           = \"1\"\n    type               = \"host\"\n  }\n  memory            = 1024\n  scsihw            = \"virtio-scsi-pci\"\n  boot              = \"order=scsi0\"\n\n  disks {\n    scsi {\n      scsi0 {\n        disk {\n          size     = \"20G\"\n          storage  = \"data\"\n        }\n      }\n    }\n  }\n\n  network {\n    id              = 0\n    model           = \"virtio\"\n    bridge          = \"vmbr0\"\n  }\n\n  lifecycle {\n    ignore_changes  = [\n      network,\n    ]\n  }\n\n  # Cloud Init Settings\n  ipconfig0         = \"ip=192.168.2.13${count.index + 1}/24,gw=192.168.2.1\"\n\n  sshkeys = &lt;&lt;EOF\n  ${var.ssh_key}\n  EOF\n}</code></pre>\n\n    <h4 id=\"add-ssh-pubkey-for-cloud-init\">\n      Add ssh-pubkey for Cloud-init\n    </h4>\n\n  <p>\n    To get passwordless login (useful for tools like Ansible), create a variable with your ssh_key in the <strong><em>variables.tf</em></strong> file:\n  </p>\n<pre class=\"line-numbers  language-json\"><code># variables.tf\n\nvariable \"ssh_key\" {\n  default = \"ssh-rsa ...\"\n}\n</code></pre>\n\n    <h3 id=\"deployment-time\">\n      Deployment time\n    </h3>\n\n  <p>\n    Terraform has a simple but powerful deployment cycle, which consists of the following steps:\n  </p>\n\n  <ul>\n    <li>Init - Initializes the Terraform project and install needed plugins, dependencies...</li><li>Validate - Validates the syntax of the created Terraform .tf files</li><li>Plan - Calculates the steps and changes to install/upgrade your infrastructure</li><li>Apply - Applies the changes on the configured systems</li>\n  </ul>\n\n  <p>\n    If you try to skip a step for example start with terraform plan, Terraform inform you to initialise the project first:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>Error: Could not satisfy plugin requirements\n\n\nPlugin reinitialization required. Please run \"terraform init\".\n\nPlugins are external binaries that Terraform uses to access and manipulate\nresources. The configuration provided requires plugins which can't be located,\ndon't satisfy the version constraints, or are otherwise incompatible.\n\nTerraform automatically discovers provider requirements from your\nconfiguration, including providers used in child modules. To see the\nrequirements and constraints from each module, run \"terraform providers\".\n\n\n\nError: provider.proxmox: new or changed plugin executable</code></pre>\n\n  <p>\n    So let's start with the initialisation first:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.</code></pre>\n\n  <p>\n    And Terraform informs you about the next step. But instead we like to check if our configuration ist correct:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>terraform validate\n\nSuccess! The configuration is valid.</code></pre>\n\n  <p>\n    Seems we did a good job during our configuration. Now it's the time to see what Terraform likes to deploy:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>terraform plan\n\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\n\n------------------------------------------------------------------------\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n.\n.\n.\n.\n\nPlan: 4 to add, 0 to change, 0 to destroy.\n\n------------------------------------------------------------------------\n\nThis plan was saved to: planfile\n\nTo perform exactly these actions, run the following command to apply:\n    terraform apply \"planfile\"\n</code></pre>\n\n  <p>\n    As you can see - Terraform likes to install 4 new server. It also shows us the detailed configuration. The configuration can be read like a \"diff\" file:\n  </p>\n\n  <ul>\n    <li>\"+\" means add</li><li>\"-\" means remove</li><li>\"~\" means replaced</li>\n  </ul>\n\n  <p>\n    The file \"planfile\" can be used for the next apply command:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>terraform apply planfile\n\nApply complete! Resources: 4 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate</code></pre>\n\n  <p>\n    Depending on your hardware this needs some time. If everything runs fine you can see the output above. Terraform successfully created 4 new ressources, which you can use for install the K3s cluster. Because we don't like to install anything manually we will use Ansible for this job. <br><br>But this is another story I have to tell ;-)<br><br>Wish all of you a happy new year!\n  </p>",
            "image": "https://norocketscience.at/media/posts/5/dominik-vanyi-Mk2ls9UBO2E-unsplash.jpg",
            "author": {
                "name": "Thomas Brandstetter"
            },
            "tags": [
                   "terraform",
                   "proxmox"
            ],
            "date_published": "2019-12-31T09:47:00+01:00",
            "date_modified": "2026-02-02T08:13:25+01:00"
        },
        {
            "id": "https://norocketscience.at/about/",
            "url": "https://norocketscience.at/about/",
            "title": "Deploy Proxmox virtual machines using Cloud-init",
            "summary": "Due to performance problems with my ESXI homelab I decided to give the open source solution Proxmox a try. One of my goals was to install all my virtual machines with the Cloud-init solution. With Cloud-init it is possible to inject informations such as ssh&hellip;",
            "content_html": "\n  <p>\n    Due to performance problems with my ESXI homelab I decided to give the open source solution Proxmox a try. One of my goals was to install all my virtual machines with the Cloud-init solution. With Cloud-init it is possible to inject informations such as ssh keys, network information or user profiles in an standarized way at boot time. The benefit of using Cloud-init is the pre-provisioning of necessary configuration items such as a static ip address or a default user with activated ssh public key authentication. Furthermore, this kind of provisioning helps a lot for further automation steps with Ansible or even complete CI/CD pipelines in Gitlab, Jenkins etc. But first, let's start with the VM provisioning steps in Proxmox.\n  </p>\n\n    <h2 id=\"location-of-cloud-init-images\">\n      Location of Cloud-Init images\n    </h2>\n\n  <p>\n    Nowadays, nearly all big Linux distributions offer ready to use Cloud-init images. Here is a short list of mirrors where you can choose the distribution that fits your needs:\n  </p>\n\n  <ul>\n    <li>Ubuntu: <a href=\"https://cloud-images.ubuntu.com\">https://cloud-images.ubuntu.com</a></li><li>Debian: <a href=\"https://cloud.debian.org/images/cloud/\">https://cloud.debian.org/images/cloud/</a></li><li>CentOs: <a href=\"https://cloud.centos.org/centos/7/images/\">https://cloud.centos.org/centos/7/images/</a></li>\n  </ul>\n\n    <h2 id=\"create-your-template\">\n      Create your template\n    </h2>\n\n  <p>\n    All commands below have to be executed on the Proxmox server. You can ssh into it or use the shell in the web interface.\n  </p>\n\n    <h3 id=\"download-the-image-on-your-proxmox-server\">\n      Download the image on your Proxmox server\n    </h3>\n\n  <p>\n    I'm using Ubuntu for my VMs\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>bash \nwget https://cloud-images.ubuntu.com/daily/server/releases/24.04/release/ubuntu-24.04-server-cloudimg-amd64.img\n</code></pre>\n\n    <h3 id=\"define-your-virtual-machine-as-templatelessbrgreater\">\n      Define your virtual machine as template<br>\n    </h3>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm create 9000 --name \"ubuntu-2404-cloudinit-template\" --memory 2048 --net0 virtio,bridge=vmbr0\n</code></pre>\n\n  <p>\n    With this command you have created a new virtual machine with the id 9000 (has to be unique in the Proxmox ecosystem), 2 gigabyte of ram and a bridge network using the virtio controller. I took vmbr0 because it is the standard bridge in Proxmox. Feel free to use another one or add additional hardware to your template.\n  </p>\n\n    <h3 id=\"import-the-disk-image-in-the-local-proxmox-storage\">\n      Import the disk image in the local Proxmox storage\n    </h3>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm importdisk 9000 ubuntu-24.04-server-cloudimg-amd64.img local-lvm\n</code></pre>\n\n  <p>\n    The command line utility copies the image in the local Proxmox storage and assigns it to the previously created template VM with the id 9000. Because it is the first disk for the vm, Proxmox is creating a disk with the naming \"vm-9000-disk-0\".\n  </p>\n\n    <h3 id=\"configure-your-virtual-machine-to-use-the-disk-image\">\n      Configure your virtual machine to use the disk image\n    </h3>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm set 9000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9000-disk-0\n</code></pre>\n\n    <h3 id=\"adding-the-cloud-init-image-as-cd-rom-to-your-virtual-machine\">\n      Adding the Cloud-init image as CD-Rom to your virtual machine\n    </h3>\n<pre class=\"line-numbers  language-bash\"><code>qm set 9000 --ide2 local-lvm:cloudinit\n</code></pre>\n\n  <p>\n    This is an important step, because it allows you to change the settings I've already mentioned before. Do not set anything else here, because we're using this virtual machine as a template. You can edit the settings after you've cloned the template for use.<br>\n  </p>\n\n    <h3 id=\"restrict-the-virtual-machine-to-boot-from-the-cloud-init-image-only\">\n      Restrict the virtual machine to boot from the Cloud-init image only\n    </h3>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm set 9000 --boot c --bootdisk scsi0</code></pre>\n\n    <h3 id=\"attach-a-serial-console-to-the-virtual-machine-this-is-needed-for-some-cloud-init-distributions-such-as-ubuntu\">\n      Attach a serial console to the virtual machine (this is needed for some Cloud-Init distributions, such as Ubuntu)\n    </h3>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm set 9000 --serial0 socket --vga serial0</code></pre>\n\n    <h3 id=\"finally-create-a-template\">\n      Finally create a template\n    </h3>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm template 9000</code></pre>\n\n    <h2 id=\"create-a-virtual-machine-out-of-the-template\">\n      Create a virtual machine out of the template\n    </h2>\n\n  <p>\n    With the template you can clone as many virtual machines as you like and change the Cloud-init parameters for your needs. First we have to clone the template to a new virtual machine:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm clone 9000 100 --name my-virtual-machine</code></pre>\n\n  <p>\n    We created a new virtual machine with the unique id 100 and the name \"my-virtual-machine\". Now you can change the Cloud-init settings either in the admin ui or with the qm command:\n  </p>\n<pre class=\"line-numbers  language-bash\"><code>bash\nqm set 100 --sshkey ~/.ssh/id_rsa.pub \nqm set 123 --ipconfig0 ip=192.168.2.100/24,gw=192.168.2.1</code></pre>\n\n  <p>\n    With this command you have set a public key for SSH authentication and the static IP 192.168.2.100. We didn't set a user which means Ubuntu is using the default one (ubuntu). That's it! Your Cloud-Init image should now boot up fine with the desired settings.\n  </p>\n\n    <h2 id=\"next-steps\">\n      Next steps\n    </h2>\n\n  <p>\n    This tutorial is just the beginning. You're now able to use Terraform, Ansible or other automation tools to create \"Infrastructure as a code\" helping you to ramp up whole datacenters with just a few commands.\n  </p>\n\n  <p>\n    But this is another story I have to tell ;-)\n  </p>\n\n  <p>\n    Source: <a href=\"https://pve.proxmox.com/wiki/Cloud-Init_Support\">https://pve.proxmox.com/wiki/Cloud-Init_Support</a>\n  </p>",
            "image": "https://norocketscience.at/media/posts/1/Untitled.jpg",
            "author": {
                "name": "Thomas Brandstetter"
            },
            "tags": [
                   "proxmox",
                   "cloud-init"
            ],
            "date_published": "2019-12-02T08:57:00+01:00",
            "date_modified": "2026-01-29T11:08:55+01:00"
        }
    ]
}

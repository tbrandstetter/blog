<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>No Rocket Science</title>
    <link href="https://norocketscience.at/feed.xml" rel="self" />
    <link href="https://norocketscience.at" />
    <updated>2026-01-30T21:23:38+01:00</updated>
    <author>
        <name>Thomas Brandstetter</name>
    </author>
    <id>https://norocketscience.at</id>

    <entry>
        <title>Highly separated Talos Kubernetes Cluster: Part1 - Overview</title>
        <author>
            <name>Thomas Brandstetter</name>
        </author>
        <link href="https://norocketscience.at/supercharged-air-gapped-talos-kubernetes-cluster-an-overview/"/>
        <id>https://norocketscience.at/supercharged-air-gapped-talos-kubernetes-cluster-an-overview/</id>
        <media:content url="https://norocketscience.at/media/posts/7/geranimo-OomNPPv1Rpk-unsplash.jpg" medium="image" />

        <updated>2026-01-29T20:46:00+01:00</updated>
            <summary type="html">
                <![CDATA[
                        <img src="https://norocketscience.at/media/posts/7/geranimo-OomNPPv1Rpk-unsplash.jpg" alt="" />
                    Welcome to the overview of a series on building an Talos Kubernetes cluster with enhanced security. This journey will take you through every step, from setting up the initial infrastructure to deploying a fully functional demo application. In this introductory post, we'll discuss why I'm&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://norocketscience.at/media/posts/7/geranimo-OomNPPv1Rpk-unsplash.jpg" class="type:primaryImage" alt="" /></p>
                
  <p>
    Welcome to the overview of a series on building an Talos Kubernetes cluster with enhanced security. This journey will take you through every step, from setting up the initial infrastructure to deploying a fully functional demo application. In this introductory post, we'll discuss why I'm undertaking this project, the benefits it offers, and outline the developed architecture as well as necessary software and hardware components.
  </p>

    <h2 id="highly-separated-not-air-gapped">
      Highly separated not air-gapped
    </h2>

    <figure class="blockquote">
      <blockquote>An air gap is a security measure that involves isolating a computer or network and preventing it from establishing an external connection. An air-gapped computer is physically segregated and incapable of connecting wirelessly or physically with other computers or network devices.</blockquote>
      <figcaption>https://www.techtarget.com/whatis/definition/air-gapping</figcaption>
    </figure>

  <p id="why-am-i-doing-it">
    In the last years the term "air-gapped" has been mistakenly used for systems who tried to minimise the attack vector through eliminating the direct connection to the internet.
  </p>

    <figure class="blockquote">
      <blockquote>In the modern world, almost nothing is air gapped in the original sense of the word, except for maybe some Safety Instrumented Systems, and the occasional CPU on a power drill, saw or other power tool. Security practitioners who use the word “air gap” to describe their connected networks are sowing confusion. Stop it.</blockquote>
      <figcaption>https://waterfall-security.com/ot-insights-center/ot-cybersecurity-insights-center/air-gaps-i-do-not-think-it-means-what-you-think-it-means/</figcaption>
    </figure>

  <p>
    So, I'm going to use the term "highly separated" for the rest of this blog series to reflect an enhanced security pattern used for this architecture.
  </p>

    <h2 id="why-am-i-doing-it">
      Why am I doing it?
    </h2>

  <p>
    In today's rapidly evolving IT landscape, security and isolation are paramount. A Kubernetes cluster within a dedicated network and internal set up registries provides an additional layer of security by moving the services away from external access points. This setup is particularly useful for organisations handling sensitive data or operating in highly regulated industries.
  </p>

    <h2 id="what-are-the-benefits">
      What are the benefits?
    </h2>

  <p>
    <u>Enhanced Security:</u> By using strict rules to move the cluster away from other networks, we reduce potential attack vectors.
  </p>

  <p>
    <u>Compliance:</u> Many regulations require strict separation of systems handling sensitive information.
  </p>

  <p>
    <u>Performance:</u> Dedicated resources can lead to more predictable and consistent performance for applications running within the cluster.
  </p>

    <h2 id="what-am-i-going-to-do">
      What am I going to do?
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://norocketscience.at/media/posts/7/Untitled-Diagram.drawio-7.png" height="1041" width="797" alt=""  sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-xs.png 640w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-sm.png 768w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-md.png 1024w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-lg.png 1366w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-xl.png 1600w ,https://norocketscience.at/media/posts/7/responsive/Untitled-Diagram.drawio-7-2xl.png 1920w">
      
    </figure>

  <p>
    So what do we see here? First of all three networks are defined in Proxmox to support the Kubernetes deployment in this lab:
  </p>

  <ul>
    <li><u>DMZ</u> - houses the HAProxy and HAProxy Ingress Controller and is used for external traffic to our example application after successful installation</li><li><u>MGMT</u> - is used for redirect necessary registry traffic and Talos internal image factory functionality. It takes care that Talos has no need to connect to the internet to get the needed images. Furthermore, the management client is installed here to have a convenient place for installation, both through cli and browser gui</li><li><u>SRV</u> - is reserved for the TALOS cluster installation only.</li>
  </ul>

  <p>
    All defined networks are secured by pfSense which takes care that only necessary and allowed traffic goes through. The detailed firewall config will be discussed in later parts of this series.
  </p>

  <p>
    Out of scope is the network out of Proxmox. The assumption is that you will have two separated lan (vlan or else) definitions to work with. One client lan to get into the management client seamlessly and one server lan where the WAN traffic from pfSense goes through and ingress traffic flows in.
  </p>

  <p>
    Based on my research the setup will involve the following software components.
  </p>

    <h3 id="proxmox-ve">
      Proxmox VE
    </h3>

  <p>
    Proxmox VE is built on open-source principles, fostering a community-driven approach that ensures continuous security, improvement and innovation. This openness is a perfect fit for the setup I like to implement. Additionally,&nbsp;Proxmox VE offers comprehensive backup and replication solutions to safeguard your data against loss or corruption.
  </p>

    <h3 id="opentofu">
      OpenTofu
    </h3>

  <p>
    For the VM proviosioning I had to decide between Terraform and OpenTofu. I've chosen OpenTofu because it is fully open-source, which means you have complete transparency into how it works. This can be crucial for security-conscious organizations. Furthermore it encourages contributions from a wider community, potentially leading to faster bug fixes and new features.
  </p>

    <h3 id="pfsense">
      pfSense
    </h3>

  <p>
    pfSense is my preferred choice for separating and controlling network traffic in several ways:
  </p>

  <p>
    <u>Network segmentation:</u>&nbsp;pfSense allows me to connect multiple, isolated network segments within a single physical network infrastructure in a virtualised environment like Proxmox. This helps prevent broadcasts between networks, reducing congestion and improving security.
  </p>

  <p>
    <u>Firewall control:</u>&nbsp;With the integrated stateful firewall, I can define granular rules for traffic flow between these segments. I can allow or block specific ports/protocols, restrict access based on source/destination IP, and even shape bandwidth to prioritize critical services.
  </p>

  <p>
    <u>Additional Services:</u>&nbsp;pfSense includes several additional features like dhcp dns resolver and BGP support. These help to bundle the needed functionality in one point.
  </p>

  <p>
    <u>Centralized Management:</u>&nbsp;pfSense provides a web-based GUI for managing all these features from a central location. This makes it easier to maintain consistency across multiple networks.
  </p>

  <p>
    <u>Cost-Effective: </u>Since the firewall is open-source, I can run it on affordable hardware or virtual machines, making it a cost-effective solution compared to commercial alternatives.
  </p>

  <p>
    In essence, pfSense gives me the tools and control I need to design this secure, efficient network architecture tailored to my specific requirements.
  </p>

    <h3 id="ubuntu">
      Ubuntu
    </h3>

  <p>
    Not much to say here. It is my preferred Linux distribution to work with. Im going to use it as a Management client to manage the entire setup from one host and it is additionally used to set up the bastion host (Harbor and HAProxy).
  </p>

    <h3 id="harbor">
      Harbor
    </h3>

  <p>
    Harbor is an open-source cloud-native registry that stores, signs, and scans container images for vulnerabilities. It provides advanced features like content signing, image replication, and role-based access control (RBAC). With Harbor, I can securely store Docker and OCI images and ensure their integrity through cryptographic signatures. Furthermore, it is possible to activate vulnerability scanning on uploaded images. These security aspects are crucial to complete the air gapped architecture for Talos. The integrated web portal offers a user-friendly interface for managing repositories, while the included API enables integration for future use CI/CD pipelines.&nbsp;
  </p>

    <h3 id="talos">
      Talos
    </h3>

  <p>
    Coming from OpenShift in my professional work environment I was stunned by Talos regarding security and simplicity. I decided to go for Talos because of:
  </p>

  <ol>
    <li><u>Minimal attack surface:</u> Talos is a minimal, immutable operating system that only includes essential components for running Kubernetes. This reduced attack surface minimises potential vulnerabilities and makes the system easier to secure.</li><li><span style="font-size: 1em;"><u>Immutable infrastructure:</u> Since Talos uses an immutable infrastructure model, changes are made through atomic updates, ensuring consistency and reducing the risk of configuration drift or human error introducing security flaws.</span></li><li><u>Simplified security updates:</u> Talos simplifies the process of patching and updating the system, which means critical security patches can be applied quickly without complex installation steps, keeping the cluster nodes secure against known vulnerabilities.</li><li><u>Strong default security posture:</u> Out of the box, Talos enforces strong security practices like read-only root filesystems, mandatory use of cryptographic signatures for updates, and strict security policies. This minimises misconfigurations that could lead to security issues.</li>
  </ol>

    <h3 id="haproxy">
      HAProxy
    </h3>

  <p>
    HAProxy is renowned for its high performance, reliability, and low resource consumption. It's designed to handle a large number of connections efficiently, making it ideal for managing incoming traffic at scale. Its advanced load balancing algorithms (round-robin, least connections, etc.) in combination with the build-in SSL/TLS support allows us to distribute traffic intelligently across backend services, ensuring optimal use of resources and improved application performance. On top of that it delivers enhanced security services like allow-/denylisting, rate limiting and DDoS protection.
  </p>

  <p>
    HAProxy as an Ingress controller integrates seamlessly with Kubernetes, enabling dynamic updates based on Kubernetes Ingress resources. This allows for easy management of application routes directly within the Kubernetes ecosystem and makes this component a vital part in the cloud native architecture.
  </p>

    <h3 id="calico">
      Calico
    </h3>

  <p>
    Calico is an open-source networking solution designed to provide low-latency, high-performance network connectivity for containers by leveraging the Linux kernel for routing packages.  Unlike some CNI solutions that rely on encapsulation (like VXLAN or IPIP), Calico can use pure Layer 3 routing. This reduces overhead and allows for better performance, especially in large-scale environments. The final and most important part is BGP. By integrating BGP (Border Gateway Protocol), Calico enables efficient routing of traffic across different networks, which is essential for the planned architecture.In the end it will be a perfect fit to the HAProxy counterpart.
  </p>

    <h2 id="next-steps">
      Next steps
    </h2>

  <p>
    Now that we nailed down the architecture and the used components, it is time to prepare the most important part of this setup.
  </p>

  <p>
    To get started, you'll need:<br>
  </p>

  <ul>
    <li>Proxmox VE installed on a physical server or virtual machine</li><li>Internet access to the necessary ISO images and software packages.</li><li>A reliable network setup with appropriate subnets and/or VLANs.</li>
  </ul>

  <p>
    In subsequent posts over the next weeks, I will delve into each component in detail. Stay tuned for our next post, where we'll guide you through preparing your Proxmox environment!
  </p>

  <p class="msg msg--info">
    Being a non native English speaker, some parts of this text have been written with the support of artificial intelligence.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Install Proxmox virtual machines with Terraform</title>
        <author>
            <name>Thomas Brandstetter</name>
        </author>
        <link href="https://norocketscience.at/install-proxmox-virtual-machines-with-terraform-2/"/>
        <id>https://norocketscience.at/install-proxmox-virtual-machines-with-terraform-2/</id>
        <media:content url="https://norocketscience.at/media/posts/5/dominik-vanyi-Mk2ls9UBO2E-unsplash.jpg" medium="image" />
            <category term="terraform"/>
            <category term="proxmox"/>

        <updated>2019-12-31T09:47:00+01:00</updated>
            <summary type="html">
                <![CDATA[
                        <img src="https://norocketscience.at/media/posts/5/dominik-vanyi-Mk2ls9UBO2E-unsplash.jpg" alt="" />
                    A few words about Terraform If you have read my last blog post you already know how to create templates for further provisioning using the Cloud-init specification. In this post I like to show you how you easily deploy your infrastructure using Terraform on the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://norocketscience.at/media/posts/5/dominik-vanyi-Mk2ls9UBO2E-unsplash.jpg" class="type:primaryImage" alt="" /></p>
                
    <h2 id="a-few-words-about-terraform">
      A few words about Terraform
    </h2>

  <p>
    If you have read my <a href="https://norocketscience.at/about/">last blog post</a> you already know how to create templates for further provisioning using the Cloud-init specification. In this post I like to show you how you easily deploy your infrastructure using Terraform on the virtualisation solution Proxmox. First of all, what is Terraform exactly? Terraform is a so called "Infrastructure as a code" software developed by <a href="https://www.hashicorp.com">Hashicorp</a> - a company which also created <a href="https://www.vagrantup.com">Vagrant</a> and other famous tools for professional cloud solutions. According to <a href="https://docs.microsoft.com/en-us/azure/devops/learn/what-is-infrastructure-as-code">Microsoft</a>,<br>
  </p>

    <blockquote class="blockquote">
      Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code.<br>
    </blockquote>

  <p>
    What's the benefit you may ask. One of the biggest advantages is that you don't have to install your servers and other components manually. Instead, with just a few lines of code you're able to manage your complete infrastructure. This automation step brings your stability, quality improvements, resilience and time for more important things. Furthermore, you have the possibility to integrate your Terraform configuration into your existing VCS and automatically run changes through your infrastructure with a proper CI/CD pipeline.<br>
  </p>

  <p>
    Another good question - why use Terraform if you've already have Ansible in place?Have you ever tried to remove a package in Ansible? If you just remove it from you playbook configuration it is still on your server until you write the right configuration (the attribute "absent" is your friend) or remove it manualy. Terraform takes care on this changes and has a clever dependency management for "First this, then that" decisions. Writing about the differences between the two would hijack my "Terraform practicing" article, so to keep a long story short:
  </p>

  <ul>
    <li>Use Terraform for all your infrastructure stuff (VMs and the baseline software on it)</li><li>Use Ansible for the configuration management (Installation of Applications and the configuration of it)</li>
  </ul>

    <h2 id="lets-start-practicing">
      Let's start practicing
    </h2>

    <h3 id="assumptions">
      Assumptions
    </h3>

  <ul>
    <li>You have already a Proxmox server installed (it works with other virtualisation and cloud providers too - but this article is about Proxmox and Terraform)</li><li>You have already OS-templates with Cloud-init defined (if not, please read my <a href="https://norocketscience.at/about/">blog post</a> about it)</li>
  </ul>

    <h3 id="installation-of-terraform">
      Installation of Terraform
    </h3>

  <p>
    You can install Terraform on the major plattforms using either the package manager/appstore from the chosen system or download the binaries from <br><a href="https://developer.hashicorp.com/terraform/install">Hashicorp</a> directly. On OSX I used <a href="https://brew.sh/">Homebrew</a>, which is a package manager for OSX.
  </p>
<pre class="line-numbers  language-bash"><code>bash
brew install terraform</code></pre>

    <h3 id="configure-your-first-terraform-project">
      Configure your first Terraform project
    </h3>

  <p>
    Now we're going to configure our first Terraform infrastructure which runs on Proxmox. In my example, I like to install a small Kubernetes infrastructure which should run <a href="https://k3s.io/">K3s (a lightweight Kubernetes distribution)</a> later on. For that <br>we have to define the following environment:<br>
  </p>

  <ul>
    <li>1 x Master Server</li><li>2 x Node Server</li><li>1 x Storage Server (for persistent storage)</li>
  </ul>

    <h4 id="configure-the-proxmox-provider">
      Configure the Proxmox provider
    </h4>

  <p>
    First, we configure the connection settings for the Proxmox provider. I chose the provider from <a href="https://github.com/Telmate/terraform-provider-proxmox">Telmate</a> for the Proxmox provisioning. For better readability of our infrastructure code, we split variables and provider in two different configuration files named <em></em><strong><em>variables.tf</em> </strong>and <strong><em>provider.tf</em></strong>
  </p>
<pre class="line-numbers  language-json"><code># variables.tf

variable "pm_api_url" {
  default = "https://your-ip:8006/api2/json"
}

variable "pm_user" {
  default = "terraform-prov@pve"
}

variable "pm_password" {
  default = "your-password"
}
</code></pre>
<pre class="line-numbers  language-json"><code># provider.tf

terraform {
  required_providers {
    proxmox = {
      source  = "telmate/proxmox"
      version = "3.0.2-rc07"
    }
  }
}

provider "proxmox" {
  pm_parallel     = 1
  pm_tls_insecure = true
  pm_api_url      = var.pm_api_url
  pm_password     = var.pm_password
  pm_user         = var.pm_user
  pm_log_enable   = true
  pm_log_file     = "terraform-plugin-proxmox.log"
  pm_debug        = true
  pm_log_levels = {
    _default    = "debug"
    _capturelog = ""
  }
}
</code></pre>

    <h4 id="configure-the-virtual-machines">
      Configure the virtual machines
    </h4>

  <p>
    Next step is the main configuration of our k3s-cluster server. Here you have to adapt the following attributes according to your configuration:
  </p>

  <ul>
    <li>target_node (the name of your Proxmox instance)</li><li>name (the name of the virtual server)</li><li>clone (the name of the template in Proxmox)</li><li>cores</li><li>memory</li><li>storage (the right storage pool in Proxmox)</li><li>ipconfig0 (Use the right IP range for your servers - the count.index is necessary if you have more then one server configured - like the k3s_agents in the example below)</li>
  </ul>

  <p>
    The "ignore changes" lifecycle block is necessary, because Terraform likes to change the mac address on the second run - maybe a problem in the Proxmox provider - see here: <a href="https://github.com/Telmate/terraform-provider-proxmox/issues/112">https://github.com/Telmate/terraform-provider-proxmox/issues/112</a>
  </p>
<pre class="line-numbers  language-json"><code># main.cf

resource "proxmox_vm_qemu" "k3s_server" {
  count             = 1
  name              = "kubernetes-master-${count.index}"
  target_node       = "homelab"

  clone             = "ubuntu-2404-cloudinit-template"

  os_type           = "cloud-init"
  cpu {
    cores             = 4
    sockets           = "1"
    type               = "host"
  }
  memory            = 1024
  scsihw            = "virtio-scsi-pci"
  boot              = "order=scsi0"

  disks {
    scsi {
      scsi0 {
        disk {
          size     = "20G"
          storage  = "data"
        }
      }
    }
  }

  network {
    id              = 0
    model           = "virtio"
    bridge          = "vmbr0"
  }

  lifecycle {
    ignore_changes  = [
      network,
    ]
  }

  # Cloud Init Settings
  ipconfig0         = "ip=192.168.2.11${count.index + 1}/24,gw=192.168.2.1"

  sshkeys = &lt;&lt;EOF
  ${var.ssh_key}
  EOF
}

resource "proxmox_vm_qemu" "k3s_agent" {
  count             = 2
  name              = "kubernetes-node-${count.index}"
  target_node       = "homelab"

  clone             = "ubuntu-2404-cloudinit-template"

  os_type           = "cloud-init"
  cpu {
    cores             = 4
    sockets           = "1"
    type               = "host"
  }
  memory            = 1024
  scsihw            = "virtio-scsi-pci"
  boot              = "order=scsi0"

  disks {
    scsi {
      scsi0 {
        disk {
          size     = "20G"
          storage  = "data"
        }
      }
    }
  }

  network {
    id              = 0
    model           = "virtio"
    bridge          = "vmbr0"
  }

  lifecycle {
    ignore_changes  = [
      network,
    ]
  }

  # Cloud Init Settings
  ipconfig0         = "ip=192.168.2.12${count.index + 1}/24,gw=192.168.2.1"

  sshkeys = &lt;&lt;EOF
  ${var.ssh_key}
  EOF
}

resource "proxmox_vm_qemu" "storage" {
  count             = 1
  name              = "storage-node-${count.index}"
  target_node       = "homelab"

  clone             = "ubuntu-2404-cloudinit-template"

  os_type           = "cloud-init"
  cpu {
    cores             = 4
    sockets           = "1"
    type               = "host"
  }
  memory            = 1024
  scsihw            = "virtio-scsi-pci"
  boot              = "order=scsi0"

  disks {
    scsi {
      scsi0 {
        disk {
          size     = "20G"
          storage  = "data"
        }
      }
    }
  }

  network {
    id              = 0
    model           = "virtio"
    bridge          = "vmbr0"
  }

  lifecycle {
    ignore_changes  = [
      network,
    ]
  }

  # Cloud Init Settings
  ipconfig0         = "ip=192.168.2.13${count.index + 1}/24,gw=192.168.2.1"

  sshkeys = &lt;&lt;EOF
  ${var.ssh_key}
  EOF
}</code></pre>

    <h4 id="add-ssh-pubkey-for-cloud-init">
      Add ssh-pubkey for Cloud-init
    </h4>

  <p>
    To get passwordless login (useful for tools like Ansible), create a variable with your ssh_key in the <strong><em>variables.tf</em></strong> file:
  </p>
<pre class="line-numbers  language-json"><code># variables.tf

variable "ssh_key" {
  default = "ssh-rsa ..."
}
</code></pre>

    <h3 id="deployment-time">
      Deployment time
    </h3>

  <p>
    Terraform has a simple but powerful deployment cycle, which consists of the following steps:
  </p>

  <ul>
    <li>Init - Initializes the Terraform project and install needed plugins, dependencies...</li><li>Validate - Validates the syntax of the created Terraform .tf files</li><li>Plan - Calculates the steps and changes to install/upgrade your infrastructure</li><li>Apply - Applies the changes on the configured systems</li>
  </ul>

  <p>
    If you try to skip a step for example start with terraform plan, Terraform inform you to initialise the project first:
  </p>
<pre class="line-numbers  language-bash"><code>Error: Could not satisfy plugin requirements


Plugin reinitialization required. Please run "terraform init".

Plugins are external binaries that Terraform uses to access and manipulate
resources. The configuration provided requires plugins which can't be located,
don't satisfy the version constraints, or are otherwise incompatible.

Terraform automatically discovers provider requirements from your
configuration, including providers used in child modules. To see the
requirements and constraints from each module, run "terraform providers".



Error: provider.proxmox: new or changed plugin executable</code></pre>

  <p>
    So let's start with the initialisation first:
  </p>
<pre class="line-numbers  language-bash"><code>terraform init

Initializing the backend...

Initializing provider plugins...

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.</code></pre>

  <p>
    And Terraform informs you about the next step. But instead we like to check if our configuration ist correct:
  </p>
<pre class="line-numbers  language-bash"><code>terraform validate

Success! The configuration is valid.</code></pre>

  <p>
    Seems we did a good job during our configuration. Now it's the time to see what Terraform likes to deploy:
  </p>
<pre class="line-numbers  language-bash"><code>terraform plan

Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.


------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

.
.
.
.

Plan: 4 to add, 0 to change, 0 to destroy.

------------------------------------------------------------------------

This plan was saved to: planfile

To perform exactly these actions, run the following command to apply:
    terraform apply "planfile"
</code></pre>

  <p>
    As you can see - Terraform likes to install 4 new server. It also shows us the detailed configuration. The configuration can be read like a "diff" file:
  </p>

  <ul>
    <li>"+" means add</li><li>"-" means remove</li><li>"~" means replaced</li>
  </ul>

  <p>
    The file "planfile" can be used for the next apply command:
  </p>
<pre class="line-numbers  language-bash"><code>terraform apply planfile

Apply complete! Resources: 4 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the `terraform show` command.

State path: terraform.tfstate</code></pre>

  <p>
    Depending on your hardware this needs some time. If everything runs fine you can see the output above. Terraform successfully created 4 new ressources, which you can use for install the K3s cluster. Because we don't like to install anything manually we will use Ansible for this job. <br><br>But this is another story I have to tell ;-)<br><br>Wish all of you a happy new year!
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Deploy Proxmox virtual machines using Cloud-init</title>
        <author>
            <name>Thomas Brandstetter</name>
        </author>
        <link href="https://norocketscience.at/about/"/>
        <id>https://norocketscience.at/about/</id>
        <media:content url="https://norocketscience.at/media/posts/1/Untitled.jpg" medium="image" />
            <category term="proxmox"/>
            <category term="cloud-init"/>

        <updated>2019-12-02T08:57:00+01:00</updated>
            <summary type="html">
                <![CDATA[
                        <img src="https://norocketscience.at/media/posts/1/Untitled.jpg" alt="" />
                    Due to performance problems with my ESXI homelab I decided to give the open source solution Proxmox a try. One of my goals was to install all my virtual machines with the Cloud-init solution. With Cloud-init it is possible to inject informations such as ssh&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://norocketscience.at/media/posts/1/Untitled.jpg" class="type:primaryImage" alt="" /></p>
                
  <p>
    Due to performance problems with my ESXI homelab I decided to give the open source solution Proxmox a try. One of my goals was to install all my virtual machines with the Cloud-init solution. With Cloud-init it is possible to inject informations such as ssh keys, network information or user profiles in an standarized way at boot time. The benefit of using Cloud-init is the pre-provisioning of necessary configuration items such as a static ip address or a default user with activated ssh public key authentication. Furthermore, this kind of provisioning helps a lot for further automation steps with Ansible or even complete CI/CD pipelines in Gitlab, Jenkins etc. But first, let's start with the VM provisioning steps in Proxmox.
  </p>

    <h2 id="location-of-cloud-init-images">
      Location of Cloud-Init images
    </h2>

  <p>
    Nowadays, nearly all big Linux distributions offer ready to use Cloud-init images. Here is a short list of mirrors where you can choose the distribution that fits your needs:
  </p>

  <ul>
    <li>Ubuntu: <a href="https://cloud-images.ubuntu.com">https://cloud-images.ubuntu.com</a></li><li>Debian: <a href="https://cloud.debian.org/images/cloud/">https://cloud.debian.org/images/cloud/</a></li><li>CentOs: <a href="https://cloud.centos.org/centos/7/images/">https://cloud.centos.org/centos/7/images/</a></li>
  </ul>

    <h2 id="create-your-template">
      Create your template
    </h2>

  <p>
    All commands below have to be executed on the Proxmox server. You can ssh into it or use the shell in the web interface.
  </p>

    <h3 id="download-the-image-on-your-proxmox-server">
      Download the image on your Proxmox server
    </h3>

  <p>
    I'm using Ubuntu for my VMs
  </p>
<pre class="line-numbers  language-bash"><code>bash 
wget https://cloud-images.ubuntu.com/daily/server/releases/24.04/release/ubuntu-24.04-server-cloudimg-amd64.img
</code></pre>

    <h3 id="define-your-virtual-machine-as-templatelessbrgreater">
      Define your virtual machine as template<br>
    </h3>
<pre class="line-numbers  language-bash"><code>bash
qm create 9000 --name "ubuntu-2404-cloudinit-template" --memory 2048 --net0 virtio,bridge=vmbr0
</code></pre>

  <p>
    With this command you have created a new virtual machine with the id 9000 (has to be unique in the Proxmox ecosystem), 2 gigabyte of ram and a bridge network using the virtio controller. I took vmbr0 because it is the standard bridge in Proxmox. Feel free to use another one or add additional hardware to your template.
  </p>

    <h3 id="import-the-disk-image-in-the-local-proxmox-storage">
      Import the disk image in the local Proxmox storage
    </h3>
<pre class="line-numbers  language-bash"><code>bash
qm importdisk 9000 ubuntu-24.04-server-cloudimg-amd64.img local-lvm
</code></pre>

  <p>
    The command line utility copies the image in the local Proxmox storage and assigns it to the previously created template VM with the id 9000. Because it is the first disk for the vm, Proxmox is creating a disk with the naming "vm-9000-disk-0".
  </p>

    <h3 id="configure-your-virtual-machine-to-use-the-disk-image">
      Configure your virtual machine to use the disk image
    </h3>
<pre class="line-numbers  language-bash"><code>bash
qm set 9000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9000-disk-0
</code></pre>

    <h3 id="adding-the-cloud-init-image-as-cd-rom-to-your-virtual-machine">
      Adding the Cloud-init image as CD-Rom to your virtual machine
    </h3>
<pre class="line-numbers  language-bash"><code>qm set 9000 --ide2 local-lvm:cloudinit
</code></pre>

  <p>
    This is an important step, because it allows you to change the settings I've already mentioned before. Do not set anything else here, because we're using this virtual machine as a template. You can edit the settings after you've cloned the template for use.<br>
  </p>

    <h3 id="restrict-the-virtual-machine-to-boot-from-the-cloud-init-image-only">
      Restrict the virtual machine to boot from the Cloud-init image only
    </h3>
<pre class="line-numbers  language-bash"><code>bash
qm set 9000 --boot c --bootdisk scsi0</code></pre>

    <h3 id="attach-a-serial-console-to-the-virtual-machine-this-is-needed-for-some-cloud-init-distributions-such-as-ubuntu">
      Attach a serial console to the virtual machine (this is needed for some Cloud-Init distributions, such as Ubuntu)
    </h3>
<pre class="line-numbers  language-bash"><code>bash
qm set 9000 --serial0 socket --vga serial0</code></pre>

    <h3 id="finally-create-a-template">
      Finally create a template
    </h3>
<pre class="line-numbers  language-bash"><code>bash
qm template 9000</code></pre>

    <h2 id="create-a-virtual-machine-out-of-the-template">
      Create a virtual machine out of the template
    </h2>

  <p>
    With the template you can clone as many virtual machines as you like and change the Cloud-init parameters for your needs. First we have to clone the template to a new virtual machine:
  </p>
<pre class="line-numbers  language-bash"><code>bash
qm clone 9000 100 --name my-virtual-machine</code></pre>

  <p>
    We created a new virtual machine with the unique id 100 and the name "my-virtual-machine". Now you can change the Cloud-init settings either in the admin ui or with the qm command:
  </p>
<pre class="line-numbers  language-bash"><code>bash
qm set 100 --sshkey ~/.ssh/id_rsa.pub 
qm set 123 --ipconfig0 ip=192.168.2.100/24,gw=192.168.2.1</code></pre>

  <p>
    With this command you have set a public key for SSH authentication and the static IP 192.168.2.100. We didn't set a user which means Ubuntu is using the default one (ubuntu). That's it! Your Cloud-Init image should now boot up fine with the desired settings.
  </p>

    <h2 id="next-steps">
      Next steps
    </h2>

  <p>
    This tutorial is just the beginning. You're now able to use Terraform, Ansible or other automation tools to create "Infrastructure as a code" helping you to ramp up whole datacenters with just a few commands.
  </p>

  <p>
    But this is another story I have to tell ;-)
  </p>

  <p>
    Source: <a href="https://pve.proxmox.com/wiki/Cloud-Init_Support">https://pve.proxmox.com/wiki/Cloud-Init_Support</a>
  </p>
            ]]>
        </content>
    </entry>
</feed>
